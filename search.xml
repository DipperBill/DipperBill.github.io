<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Params and FLOPs</title>
      <link href="/2022/07/13/params-and-flops/"/>
      <url>/2022/07/13/params-and-flops/</url>
      
        <content type="html"><![CDATA[<h2 id="Params-and-FLOPs"><a href="#Params-and-FLOPs" class="headerlink" title="Params and FLOPs"></a>Params and FLOPs</h2><p>原文链接：<a href="https://blog.csdn.net/mzpmzk/article/details/82976871">https://blog.csdn.net/mzpmzk/article/details/82976871</a></p><h3 id="参数数量（Params）"><a href="#参数数量（Params）" class="headerlink" title="参数数量（Params）"></a>参数数量（Params）</h3><p>指模型含有多少参数，直接决定模型的大小，也影响推断时对内存的占用量<br>单位通常为M，通常参数用 float32 表示，所以模型大小是参数数量的4倍左右<br>参数数量与模型大小转换示例：10M float32 bit &#x3D; 10M × 4 Byte &#x3D; 40MB （注意位 bit 和字节 Byte 的区别）</p><h3 id="理论计算量（FLOPs）"><a href="#理论计算量（FLOPs）" class="headerlink" title="理论计算量（FLOPs）"></a>理论计算量（FLOPs）</h3><p>指模型推断时需要多少计算次数，是<strong>floating point operations</strong>的缩写（注意 s 小写），可以用来衡量算法&#x2F;模型的复杂度，这关系到算法速度，大模型的单位通常为 G（GFLOPs：10亿次浮点运算），小模型单位通常为 M<br>通常只考虑**乘加操作(Multi-Adds)**的数量，而且只考虑 CONV 和 FC 等参数层的计算量，忽略 BN 和 PReLU 等等。一般情况，CONV 和 FC 层也会忽略仅纯加操作的计算量，如 bias 偏置加和 shotcut 残差加等，目前有 BN 的卷积层可以不加 bias<br>PS：也有用 <strong>MAC（Memory Access Cost）</strong> 表示的</p><h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><p>假设卷积核大小为$K_h \times K_w$，输入通道数为$C_{in}$，输出通道数为$C_{out}$，输出特征图的宽和高分别为$W$ 和$H$，这里忽略偏置项</p><p><strong>CONV标准卷积层</strong></p><p>Params: $K_h \times K_w \times C_{in} \times C_{out}$</p><p>FLOPs: $K_h \times K_w \times C_{in} \times C_{out} \times H \times W &#x3D; params \times H \times W$</p><p><strong>FC 全连接层（相当于 k&#x3D;1）</strong></p><p>Params: $C_{in} \times C_{out}$</p><p>FLOPs: $C_{in} \times C_{out}$</p><p><a href="https://github.com/Lyken17/pytorch-OpCounter">参数量与计算量参考网址：https://github.com/Lyken17/pytorch-OpCounter</a></p><h3 id="How-to-install"><a href="#How-to-install" class="headerlink" title="How to install"></a><strong>How to install</strong></h3><p><code>pip install thop</code> or <code>pip install --upgrade git+https://github.com/Lyken17/pytorch-OpCounter.git</code></p><h3 id="How-to-use"><a href="#How-to-use" class="headerlink" title="How to use"></a><strong>How to use</strong></h3><p>basic usage</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">import</span> resnet50<span class="token keyword">from</span> thop <span class="token keyword">import</span> profilemodel <span class="token operator">=</span> resnet50<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span>macs<span class="token punctuation">,</span> params <span class="token operator">=</span> profile<span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token operator">=</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>Define the rule for 3rd party module.</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">YourModule</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># your definition</span><span class="token keyword">def</span> <span class="token function">count_your_model</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># your rule here</span><span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">)</span>macs<span class="token punctuation">,</span> params <span class="token operator">=</span> profile<span class="token punctuation">(</span>model<span class="token punctuation">,</span> inputs<span class="token operator">=</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">,</span>                         custom_ops<span class="token operator">=</span><span class="token punctuation">&#123;</span>YourModule<span class="token punctuation">:</span> count_your_model<span class="token punctuation">&#125;</span><span class="token punctuation">)</span></code></pre><p>Improve the output readability</p><p>Call <code>thop.clever_format</code> to give a better format of the output.</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> thop <span class="token keyword">import</span> clever_formatmacs<span class="token punctuation">,</span> params <span class="token operator">=</span> clever_format<span class="token punctuation">(</span><span class="token punctuation">[</span>macs<span class="token punctuation">,</span> params<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"%.3f"</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Params Calculating </tag>
            
            <tag> FLOPs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MobileNetV2</title>
      <link href="/2022/07/12/mobilenetv2/"/>
      <url>/2022/07/12/mobilenetv2/</url>
      
        <content type="html"><![CDATA[<h2 id="MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks"><a href="#MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks" class="headerlink" title="MobileNetV2: Inverted Residuals and Linear Bottlenecks"></a>MobileNetV2: Inverted Residuals and Linear Bottlenecks</h2><h3 id="MobileNetV1遗留的问题"><a href="#MobileNetV1遗留的问题" class="headerlink" title="MobileNetV1遗留的问题"></a>MobileNetV1遗留的问题</h3><h4 id="1、V1的结构问题："><a href="#1、V1的结构问题：" class="headerlink" title="1、V1的结构问题："></a>1、V1的结构问题：</h4><p>MobileNet v1 的结构其实非常简单，论文里是一个非常复古的直筒结构，类似于VGG一样。这种结构的性价比其实不高，后续一系列的 ResNet, DenseNet 等结构已经证明通过复用图像特征，使用 Concat&#x2F;Eltwise+ 等操作进行融合，能极大提升网络的性价比。</p><h4 id="2、Depthwise-Convolution的潜在问题："><a href="#2、Depthwise-Convolution的潜在问题：" class="headerlink" title="2、Depthwise Convolution的潜在问题："></a>2、Depthwise Convolution的潜在问题：</h4><p>MobileNet v1首次引入了深度可分离卷积，Depthwise Conv确实是大大降低了计算量，而且N×N Depthwise +1×1PointWise的结构在性能上也能接近N×N Conv。在实际使用的时候，我们发现Depthwise部分的kernel有不少是空的。当时我们认为，Depthwise每个kernel_dim相对于普通Conv要小得多，过小的kernel_dim，加上ReLU的激活影响下，使得神经元输出很容易变为0，所以就<strong>学废了</strong>。ReLU对于0的输出的梯度为0，所以一旦陷入0输出，就没法恢复了。我们还发现，这个问题在定点化低精度训练的时候会进一步放大。</p><h3 id="Linear-Bottleneck"><a href="#Linear-Bottleneck" class="headerlink" title="Linear Bottleneck"></a>Linear Bottleneck</h3><p>为了解决信息损失问题，v2的解决方案是直接将每个Bottleneck最后一层的ReLU6换成线性函数，具体为v2网络中就是将最后的Point-Wise卷积的ReLU6都换成线性函数，也即是Linear Bottleneck。</p><p><img src="/2022/07/12/mobilenetv2/v1hev2decharelu6.jpg" alt="ReLU6 -&gt; Linear"></p><h3 id="Expansion-layer"><a href="#Expansion-layer" class="headerlink" title="Expansion layer"></a>Expansion layer</h3><p>通过1x1PW卷积层进行升维</p><p><img src="/2022/07/12/mobilenetv2/mbnv2zaidwqianzuodsaq.png" alt="Expansion Layer"></p><p>定义升维系数<strong>t</strong></p><h3 id="Inverted-Residuals"><a href="#Inverted-Residuals" class="headerlink" title="Inverted Residuals"></a>Inverted Residuals</h3><p>MobileNet V1虽然采取了深度可分离卷积，但其网络主体仍然是VGG的直筒型结构。MobieNet V2借鉴了ResNet的残差结构，在v1网络结构基础上加入了跳跃连接。相较于ResNet的残差块结构，v2给这种结构命名为Inverted resdiual block，即倒残差块。倒残差主要体现在</p><p><img src="/2022/07/12/mobilenetv2/resnethembnet2duibi.png" alt="Resnet vs MobileNetV2"></p><h3 id="Architechture-of-MobileNetV2"><a href="#Architechture-of-MobileNetV2" class="headerlink" title="Architechture of MobileNetV2"></a>Architechture of MobileNetV2</h3><p><img src="/2022/07/12/mobilenetv2/mbnetv2architechture.png" alt="Architechture of MobileNetV2"></p><p><em>t</em> 是输入通道的倍增系数（即中间部分的通道数是输入通道数的多少倍）</p><p><em>n</em> 是该模块重复次数</p><p><em>c</em> 是输出通道数</p><p><em>s</em> 是该模块第一次重复时的 stride（之后重复都是 stride 1）</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>1.实时性和精度得到较好的平衡。</p><p>2.对于低维空间而言，进行线性映射会保存特征，而非线性映射会破坏特征。</p>]]></content>
      
      
      <categories>
          
          <category> Essay </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MobileNet </tag>
            
            <tag> DepthWise Conv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MobileNetV1</title>
      <link href="/2022/07/09/mobilenetv1/"/>
      <url>/2022/07/09/mobilenetv1/</url>
      
        <content type="html"><![CDATA[<h2 id="MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications"><a href="#MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications" class="headerlink" title="MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"></a>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</h2><h3 id="MobileNet-v1"><a href="#MobileNet-v1" class="headerlink" title="MobileNet v1"></a>MobileNet v1</h3><p>简单来说，MobileNet v1将常规卷积替换为深度可分离卷积（<strong>depthWise convolution</strong>）的VGG网络</p><p><img src="/2022/07/09/mobilenetv1/mobilenetvsvgg.png" alt="Left: Conventional Conv, Right: DepthWise Conv"></p><p>从上图中可以看出，VGG的卷积块就是1个常规3*3卷积、1个BN、1个ReLU激活层；MobileNet v1则是1个3*3深度可分离卷积和1个1*1卷积，后面分别跟着1个BN和1个ReLU激活函数。需要注意的是，MobileNet v1的ReLU指的是<strong>ReLU6</strong>，ReLU6对激活输出做了一个输出限幅，使得Max Output不超过6，<strong>目的是防止过大的激活输出值带来较大的精度损失</strong>。</p><h3 id="Depthwise-Convolution"><a href="#Depthwise-Convolution" class="headerlink" title="Depthwise Convolution"></a>Depthwise Convolution</h3><p>从维度的角度看，卷积核可以看成是一个空间维(Width and Height)和通道维的组合，而卷积操作则可以视为空间相关性和通道相关性的联合映射。从inception的1*1卷积来看，卷积中的空间相关性和通道相关性是可以解耦的，将它们分开进行映射，可能会达到更好的效果。</p><p>深度可分离卷积是在1*1卷积基础上的一种创新。主要包括两个部分：深度卷积和1*1卷积。深度卷积的目的在于对输入的每一个通道都单独使用一个卷积核对其进行卷积，也就是通道分离后再组合。1*1卷积的目的则在于加强深度。下面以一个例子来看一下深度可分离卷积。</p><p>假设我们用128个3*3*3的filter对一个7*7*3的输入进行卷积，可得到5*5*128的输出。如下图所示：</p><p><img src="/2022/07/09/mobilenetv1/putongcnn.png" alt="Conventional Convolution"></p><p>其计算量为5*5*128*3*3*3&#x3D;86400。</p><p>现在看如何使用深度可分离卷积来实现同样的结果。深度可分离卷积的第一步是深度卷积（Depth-Wise）。这里的深度卷积，就是<strong>分别用3个3*3*1的滤波器对输入的3个通道分别做卷积</strong>，也就是说要做3次卷积，每次卷积都有一个5*5*1的输出，组合在一起便是5*5*3的输出。</p><p>现在为了拓展深度达到128，我们需要执行深度可分离卷积的第二步：1x1卷积（Point-Wise）。现在我们用128个1*1*3的滤波器对5*5*3进行卷积，就可以得到5*5*128的输出。完整过程如下图所示：</p><p><img src="/2022/07/09/mobilenetv1/depthwisejuanjinie.png" alt="Depthwise Convolution"></p><p>第一步深度卷积的计算量：5*5*1*3*3*1*3&#x3D;675。第二步1x1卷积的计算量：5*5*128*1*1*3&#x3D;9600，合计计算量为10275次。</p><p>相同的卷积计算输出，深度可分离卷积要比常规卷积节省12倍的计算成本。所以<strong>深度可分离卷积是MobileNet v1能够轻量化的关键原因</strong>。</p><h3 id="MobileNet-v1-Body-Architecture"><a href="#MobileNet-v1-Body-Architecture" class="headerlink" title="MobileNet v1 Body Architecture"></a>MobileNet v1 Body Architecture</h3><p><img src="/2022/07/09/mobilenetv1/mobilenetv1jiegou.png" alt="MobileNet v1 Body Architecture"></p><h3 id="Comparison-with-VGG-16-and-Googlenetl"><a href="#Comparison-with-VGG-16-and-Googlenetl" class="headerlink" title="Comparison with VGG 16 and Googlenetl"></a>Comparison with VGG 16 and Googlenetl</h3><p><img src="/2022/07/09/mobilenetv1/mobilenet1debijiao.png" alt="Testing on ImageNet acc"></p><p>相比于VGG 16，MobileNet v1精度损失极小的前提下，参数量几乎减少为VGG 16的1&#x2F;32。</p>]]></content>
      
      
      <categories>
          
          <category> Essay </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MobileNet </tag>
            
            <tag> DepthWise Conv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Image Data Processing with CNN training</title>
      <link href="/2022/07/07/image-data-processing-augmentation/"/>
      <url>/2022/07/07/image-data-processing-augmentation/</url>
      
        <content type="html"><![CDATA[<h2 id="Dividing-Training-Set-and-Validation-Set"><a href="#Dividing-Training-Set-and-Validation-Set" class="headerlink" title="Dividing Training Set and Validation Set"></a>Dividing Training Set and Validation Set</h2><h3 id="Pre-Start-building-a-folder"><a href="#Pre-Start-building-a-folder" class="headerlink" title="Pre: Start building a folder"></a>Pre: Start building a folder</h3><p>Because the original data is messy, so we <strong>start a new dataset path .&#x2F;data</strong> to copy the <strong>Train Sets</strong> in original data.</p><pre class="language-python" data-language="python"><code class="language-python">!mkdir data!cp <span class="token punctuation">.</span><span class="token operator">/</span>datasets<span class="token operator">/</span>meowmeowmeowmeowmeow<span class="token operator">-</span>gtsrb<span class="token operator">-</span>german<span class="token operator">-</span>traffic<span class="token operator">-</span>sign<span class="token operator">-</span>momodel<span class="token operator">/</span>train <span class="token punctuation">.</span><span class="token operator">/</span>data<span class="token operator">/</span>train_images <span class="token operator">-</span>r</code></pre><h3 id="Slicing-data"><a href="#Slicing-data" class="headerlink" title="Slicing data"></a>Slicing data</h3><p>每个图片名称前六位表示类别，中间6位数表示组数，后6位表示组内序号，一组有30张图片</p><p>Divide the first three groups into validation sets</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">initialize_data</span><span class="token punctuation">(</span>folder<span class="token punctuation">)</span><span class="token punctuation">:</span>          <span class="token comment"># make validation_data by using images 00000*, 00001* and 00002* in each class</span>    train_folder <span class="token operator">=</span> folder <span class="token operator">+</span> <span class="token string">'/train_images'</span>    val_folder <span class="token operator">=</span> folder <span class="token operator">+</span> <span class="token string">'/val_images'</span>    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span>val_folder<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>val_folder <span class="token operator">+</span> <span class="token string">' not found, making a validation set'</span><span class="token punctuation">)</span>        os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>val_folder<span class="token punctuation">)</span>        <span class="token keyword">for</span> dirs <span class="token keyword">in</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>train_folder<span class="token punctuation">)</span><span class="token punctuation">:</span>            os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>val_folder <span class="token operator">+</span> <span class="token string">'/'</span> <span class="token operator">+</span> dirs<span class="token punctuation">)</span>            <span class="token keyword">for</span> f <span class="token keyword">in</span> os<span class="token punctuation">.</span>listdir<span class="token punctuation">(</span>train_folder <span class="token operator">+</span> <span class="token string">'/'</span> <span class="token operator">+</span> dirs<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">if</span> f<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token number">11</span><span class="token punctuation">]</span><span class="token operator">==</span><span class="token punctuation">(</span><span class="token string">'00000'</span><span class="token punctuation">)</span> <span class="token keyword">or</span> f<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token number">11</span><span class="token punctuation">]</span><span class="token operator">==</span><span class="token punctuation">(</span><span class="token string">'00001'</span><span class="token punctuation">)</span> <span class="token keyword">or</span> f<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token number">11</span><span class="token punctuation">]</span><span class="token operator">==</span><span class="token punctuation">(</span><span class="token string">'00002'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    <span class="token comment"># move file to validation folder</span>                    os<span class="token punctuation">.</span>rename<span class="token punctuation">(</span>train_folder <span class="token operator">+</span> <span class="token string">'/'</span> <span class="token operator">+</span> dirs <span class="token operator">+</span> <span class="token string">'/'</span> <span class="token operator">+</span> f<span class="token punctuation">,</span> val_folder <span class="token operator">+</span> <span class="token string">'/'</span> <span class="token operator">+</span> dirs <span class="token operator">+</span> <span class="token string">'/'</span> <span class="token operator">+</span> f<span class="token punctuation">)</span>data_path <span class="token operator">=</span> <span class="token string">"./data"</span>initialize_data<span class="token punctuation">(</span>data_path<span class="token punctuation">)</span></code></pre><h3 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms<span class="token comment"># data augmentation for training and test time</span><span class="token comment"># Resize all images to 256 * 256 and normalize them to mean = 0 and standard-deviation = 1 based on statistics collected from the training set</span>data_transforms <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>RandomCrop<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment">#先四周填充0，再将图像随机裁剪成32*32</span>    <span class="token comment"># transforms.RandomRotation(degrees=15),#随机旋转</span>    <span class="token comment"># transforms.RandomHorizontalFlip(),#随机水平翻转</span>    <span class="token comment"># transforms.CenterCrop(224),#中心裁剪到224*224</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.3337</span><span class="token punctuation">,</span> <span class="token number">0.3064</span><span class="token punctuation">,</span> <span class="token number">0.3171</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span> <span class="token number">0.2672</span><span class="token punctuation">,</span> <span class="token number">0.2564</span><span class="token punctuation">,</span> <span class="token number">0.2629</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    Cutout<span class="token punctuation">(</span>n_holes<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> length<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>val_transforms <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token comment"># transforms.CenterCrop(224),#中心裁剪到224*224</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.3337</span><span class="token punctuation">,</span> <span class="token number">0.3064</span><span class="token punctuation">,</span> <span class="token number">0.3171</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span> <span class="token number">0.2672</span><span class="token punctuation">,</span> <span class="token number">0.2564</span><span class="token punctuation">,</span> <span class="token number">0.2629</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># Resize, normalize and jitter image brightness</span>data_jitter_brightness <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>    transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment">#     transforms.ColorJitter(brightness=-5),</span>    transforms<span class="token punctuation">.</span>ColorJitter<span class="token punctuation">(</span>brightness<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.3337</span><span class="token punctuation">,</span> <span class="token number">0.3064</span><span class="token punctuation">,</span> <span class="token number">0.3171</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span> <span class="token number">0.2672</span><span class="token punctuation">,</span> <span class="token number">0.2564</span><span class="token punctuation">,</span> <span class="token number">0.2629</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id="Processing-data"><a href="#Processing-data" class="headerlink" title="Processing data"></a>Processing data</h3><p>Pay attention on <strong>transform</strong></p><p>And <strong>num_workers</strong> may occur errors, so we need to watch its value.</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">processing_data</span><span class="token punctuation">(</span>data_path<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>use_gpu<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">import</span> torch    <span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms    <span class="token triple-quoted-string string">"""    数据处理    :param data_path: 数据集路径    :return:  train_loader,val_loader:处理后的训练集,验证集loader    """</span>    <span class="token comment">#这里是把字典序转为按数字大小排列的序号 比如10这个文件夹的图片，原本通过datasets.ImageFolder获取到的label是2（按字典序排列 0，1，10，11，12，13...），现在转换为10</span>    <span class="token keyword">def</span> <span class="token function">char_order2int_order</span><span class="token punctuation">(</span>charoder<span class="token punctuation">)</span><span class="token punctuation">:</span>        a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">43</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            a<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span>        a<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span>        kv<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span> <span class="token punctuation">(</span><span class="token number">43</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            kv<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> kv<span class="token punctuation">[</span>charoder<span class="token punctuation">]</span>    train_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>        torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>ConcatDataset<span class="token punctuation">(</span><span class="token punctuation">[</span>datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span>data_path <span class="token operator">+</span> <span class="token string">'/train_images'</span><span class="token punctuation">,</span>                                                transform<span class="token operator">=</span>data_transforms<span class="token punctuation">,</span>                                        target_transform<span class="token operator">=</span>char_order2int_order<span class="token punctuation">)</span><span class="token punctuation">,</span>                    datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span>data_path <span class="token operator">+</span> <span class="token string">'/train_images'</span><span class="token punctuation">,</span>                     transform<span class="token operator">=</span>data_jitter_brightness<span class="token punctuation">,</span>                                            target_transform<span class="token operator">=</span>char_order2int_order<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>          batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span>          shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> pin_memory<span class="token operator">=</span>use_gpu    <span class="token punctuation">)</span>    val_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>        datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span>data_path <span class="token operator">+</span> <span class="token string">'/val_images'</span><span class="token punctuation">,</span>                             transform<span class="token operator">=</span>val_transforms<span class="token punctuation">,</span>                             target_transform<span class="token operator">=</span>char_order2int_order<span class="token punctuation">)</span><span class="token punctuation">,</span>         batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span>        shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>         num_workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>         pin_memory<span class="token operator">=</span>use_gpu     <span class="token punctuation">)</span>    <span class="token keyword">return</span> train_loader<span class="token punctuation">,</span>val_loader</code></pre><h3 id="Define-a-net-and-Train-it"><a href="#Define-a-net-and-Train-it" class="headerlink" title="Define a net and Train it"></a>Define a net and Train it</h3><p>Take simple CNN net as an example.</p><p>This CNN net is modified from <strong>VGG</strong>, but it has smaller depth.</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> Fnclasses <span class="token operator">=</span> <span class="token number">43</span>  <span class="token comment"># GTSRB as 43 classes</span><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># block 1:         3 x 32 x 32 --> 64 x 16 x 16        </span>        self<span class="token punctuation">.</span>conv1a <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>   <span class="token number">64</span><span class="token punctuation">,</span>  kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span>  <span class="token number">64</span><span class="token punctuation">,</span>  kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool1  <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># block 2:         64 x 16 x 16 --> 128 x 8 x 8</span>        self<span class="token punctuation">.</span>conv2a <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span>  <span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool2  <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># block 3:         128 x 8 x 8 --> 256 x 4 x 4        </span>        self<span class="token punctuation">.</span>conv3a <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv3b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool3  <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>                <span class="token comment">#block 4:          256 x 4 x 4 --> 512 x 2 x 2</span>        self<span class="token punctuation">.</span>conv4a <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool4  <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># linear layers:   512 x 2 x 2 --> 2048 --> 4096 --> 4096 --> 43</span>        self<span class="token punctuation">.</span>linear1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linear2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span><span class="token number">4096</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linear3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">43</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># block 1:         3 x 32 x 32 --> 64 x 16 x 16</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1a<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1b<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># block 2:         64 x 16 x 16 --> 128 x 8 x 8</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2a<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2b<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># block 3:         128 x 8 x 8 --> 256 x 4 x 4</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3a<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3b<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment">#block 4:          256 x 4 x 4 --> 512 x 2 x 2</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv4a<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># linear layers:   512 x 2 x 2 --> 2048 --> 4096 --> 4096 --> 43</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                 <span class="token keyword">return</span> x</code></pre><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token keyword">import</span> torchvision<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> tqdm <span class="token keyword">import</span> tqdm<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    correct <span class="token operator">=</span> <span class="token number">0</span>    training_loss <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>data<span class="token punctuation">,</span> target<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>tqdm<span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        data<span class="token punctuation">,</span> target <span class="token operator">=</span> Variable<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">,</span> Variable<span class="token punctuation">(</span>target<span class="token punctuation">)</span>        <span class="token keyword">if</span> use_gpu<span class="token punctuation">:</span>            data <span class="token operator">=</span> data<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>            target <span class="token operator">=</span> target<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        output <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>        mycrit<span class="token operator">=</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> mycrit<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        max_index <span class="token operator">=</span> output<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>max_index <span class="token operator">==</span> target<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        training_loss <span class="token operator">+=</span> loss    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\nTraining set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>        training_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">,</span> correct<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token number">100.</span> <span class="token operator">*</span> correct <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> training_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">100.</span> <span class="token operator">*</span> correct <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">validation</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    validation_loss <span class="token operator">=</span> <span class="token number">0</span>    correct <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> target<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>tqdm<span class="token punctuation">(</span>val_loader<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            data<span class="token punctuation">,</span> target <span class="token operator">=</span> Variable<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">,</span> Variable<span class="token punctuation">(</span>target<span class="token punctuation">)</span>            <span class="token keyword">if</span> use_gpu<span class="token punctuation">:</span>                data <span class="token operator">=</span> data<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>                target <span class="token operator">=</span> target<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>            output <span class="token operator">=</span> model<span class="token punctuation">(</span>data<span class="token punctuation">)</span>            mycrit<span class="token operator">=</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>            validation_loss <span class="token operator">+=</span> mycrit<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># sum up batch loss</span>            pred <span class="token operator">=</span> output<span class="token punctuation">.</span>data<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment"># get the index of the max log-probability</span>            correct <span class="token operator">+=</span> pred<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>target<span class="token punctuation">.</span>data<span class="token punctuation">.</span>view_as<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#     scheduler.step(np.around(validation_loss, 2))</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\nValidation set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>        validation_loss<span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>val_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">,</span> correct<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>val_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token number">100.</span> <span class="token operator">*</span> correct <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>val_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> validation_loss<span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>val_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">100.</span> <span class="token operator">*</span> correct <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>val_loader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span></code></pre><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># import argparse</span><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> time<span class="token comment">#参数设置</span><span class="token keyword">class</span> <span class="token class-name">args</span><span class="token punctuation">:</span>    data <span class="token operator">=</span> <span class="token string">"./data"</span>    batch_size <span class="token operator">=</span> <span class="token number">64</span>    epochs <span class="token operator">=</span> <span class="token number">2</span>    lr <span class="token operator">=</span> <span class="token number">0.0001</span>    seed <span class="token operator">=</span> <span class="token number">1</span>    log_interval <span class="token operator">=</span> <span class="token number">10</span>    torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span>args<span class="token punctuation">.</span>seed<span class="token punctuation">)</span><span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    use_gpu <span class="token operator">=</span> <span class="token boolean">True</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Using GPU"</span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>use_gpu <span class="token operator">=</span> <span class="token boolean">False</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Using CPU"</span><span class="token punctuation">)</span>train_loader<span class="token punctuation">,</span>val_loader<span class="token operator">=</span>processing_data<span class="token punctuation">(</span>args<span class="token punctuation">.</span>data<span class="token punctuation">,</span>args<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span>use_gpu<span class="token punctuation">)</span>model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 输出的类别数目</span><span class="token comment">#优化器和学习率调整</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span><span class="token builtin">filter</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> p<span class="token punctuation">:</span> p<span class="token punctuation">.</span>requires_grad<span class="token punctuation">,</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span>args<span class="token punctuation">.</span>lr<span class="token punctuation">)</span>scheduler <span class="token operator">=</span> optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>ReduceLROnPlateau<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> <span class="token string">'min'</span><span class="token punctuation">,</span>patience<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>factor<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>res<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token string">"loss"</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">"val_loss"</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">"accuracy"</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">"val_accuracy"</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span class="token comment">#开始训练</span>start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">if</span> use_gpu<span class="token punctuation">:</span>    model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> args<span class="token punctuation">.</span>epochs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    loss<span class="token punctuation">,</span>acc<span class="token operator">=</span>train<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>    res<span class="token punctuation">[</span><span class="token string">"loss"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    res<span class="token punctuation">[</span><span class="token string">"accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>acc<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    loss<span class="token punctuation">,</span>acc<span class="token operator">=</span>validation<span class="token punctuation">(</span><span class="token punctuation">)</span>    res<span class="token punctuation">[</span><span class="token string">"val_loss"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>    res<span class="token punctuation">[</span><span class="token string">"val_accuracy"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>acc<span class="token punctuation">)</span>    model_file <span class="token operator">=</span> <span class="token string">'results/model_'</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>epoch<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'.pth'</span>    torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> model_file<span class="token punctuation">,</span>_use_new_zipfile_serialization<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'\nSaved model to '</span> <span class="token operator">+</span> model_file <span class="token punctuation">)</span>          <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"模型训练总时长："</span><span class="token punctuation">,</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span>start<span class="token punctuation">)</span><span class="token keyword">import</span> pickle<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"./results/res"</span><span class="token punctuation">,</span><span class="token string">"wb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>res<span class="token punctuation">,</span>f<span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Image Data Processing and Augmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Summary of Tensors Function</title>
      <link href="/2022/07/02/summary-of-tensor-function-in-pytorch/"/>
      <url>/2022/07/02/summary-of-tensor-function-in-pytorch/</url>
      
        <content type="html"><![CDATA[<h2 id="Summary-of-function-in-Pytorch"><a href="#Summary-of-function-in-Pytorch" class="headerlink" title="Summary of function in Pytorch"></a>Summary of function in Pytorch</h2><h3 id="torch-stack-a-b-dim"><a href="#torch-stack-a-b-dim" class="headerlink" title="torch.stack(a, b, dim)"></a>torch.stack(a, b, dim)</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># 假设是时间步T1的输出</span>T1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>               <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>               <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 假设是时间步T2的输出</span>T2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">70</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>T1<span class="token punctuation">,</span>T2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token comment"># torch.Size([2, 3, 3])</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>T1<span class="token punctuation">,</span>T2<span class="token punctuation">)</span><span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># tensor([[[ 1,  2,  3],</span>             <span class="token punctuation">[</span> <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             <span class="token punctuation">[</span> <span class="token number">7</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             <span class="token punctuation">[</span><span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             <span class="token punctuation">[</span><span class="token number">70</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>T1<span class="token punctuation">,</span>T2<span class="token punctuation">)</span><span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># [3, 2, 3]</span><span class="token comment"># tensor([[[ 1,  2,  3],</span>             <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             <span class="token punctuation">[</span><span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">7</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             <span class="token punctuation">[</span><span class="token number">70</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">(</span>T1<span class="token punctuation">,</span>T2<span class="token punctuation">)</span><span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># [3, 3, 2]</span><span class="token comment"># tensor([[[ 1, 10],</span>             <span class="token punctuation">[</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             <span class="token punctuation">[</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             <span class="token punctuation">[</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>            <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">70</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             <span class="token punctuation">[</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">]</span><span class="token punctuation">,</span>             <span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id="torch-cat-tensors-dim-x3D-0"><a href="#torch-cat-tensors-dim-x3D-0" class="headerlink" title="torch.cat(tensors, dim&#x3D;0)"></a>torch.cat(tensors, dim&#x3D;0)</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> xtensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.6580</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0969</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4614</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1034</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5790</span><span class="token punctuation">,</span>  <span class="token number">0.1497</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.6580</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0969</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4614</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1034</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5790</span><span class="token punctuation">,</span>  <span class="token number">0.1497</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.6580</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0969</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4614</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1034</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5790</span><span class="token punctuation">,</span>  <span class="token number">0.1497</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.6580</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0969</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4614</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1034</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5790</span><span class="token punctuation">,</span>  <span class="token number">0.1497</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.6580</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0969</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4614</span><span class="token punctuation">,</span>  <span class="token number">0.6580</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0969</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4614</span><span class="token punctuation">,</span>  <span class="token number">0.6580</span><span class="token punctuation">,</span>         <span class="token operator">-</span><span class="token number">1.0969</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4614</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.1034</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5790</span><span class="token punctuation">,</span>  <span class="token number">0.1497</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1034</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5790</span><span class="token punctuation">,</span>  <span class="token number">0.1497</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1034</span><span class="token punctuation">,</span>         <span class="token operator">-</span><span class="token number">0.5790</span><span class="token punctuation">,</span>  <span class="token number">0.1497</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id="torch-unsqueeze"><a href="#torch-unsqueeze" class="headerlink" title="torch.unsqueeze()"></a>torch.unsqueeze()</h3><pre class="language-python" data-language="python"><code class="language-python">data1<span class="token operator">=</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>data1<span class="token punctuation">,</span>data1<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>data1<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># tensor([[ 1.1373,  0.1755, -0.3572],</span>          <span class="token punctuation">[</span> <span class="token number">0.3606</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4550</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0797</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># torch.Size([2, 3])</span>data2<span class="token operator">=</span>torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>data1<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>data2<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>data2<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>data2<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># tensor([[[ 1.1373,  0.1755, -0.3572],</span>             <span class="token punctuation">[</span> <span class="token number">0.3606</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4550</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0797</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># torch.Size([1, 2, 3])</span>data3<span class="token operator">=</span>torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>data1<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>data3<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>data3<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>data3<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># tensor([[[1.1373,  0.1755, -0.3572]],</span>            <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.3606</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4550</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0797</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># torch.Size([2, 1, 3])</span>data4<span class="token operator">=</span>torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>data1<span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>data4<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>data4<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>data4<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># tensor([[[ 1.1373],</span>           <span class="token punctuation">[</span> <span class="token number">0.1755</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">0.3572</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.3606</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">0.4550</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">1.0797</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># torch.Size([2, 3, 1])</span></code></pre><h3 id="torch-repeat-interleave-input-repeats-dim"><a href="#torch-repeat-interleave-input-repeats-dim" class="headerlink" title="torch.repeat_interleave(input, repeats, dim)"></a>torch.repeat_interleave(input, repeats, dim)</h3><pre class="language-python" data-language="python"><code class="language-python"><span class="token builtin">input</span> <span class="token punctuation">(</span>类型：torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span>：输入张量repeats（类型：<span class="token builtin">int</span>或torch<span class="token punctuation">.</span>Tensor）：每个元素的重复次数。repeats参数会被广播来适应输入张量的维度dim（类型：<span class="token builtin">int</span>）需要重复的维度。默认情况下，将把输入张量展平（flatten）为向量，然后将每个元素重复repeats次，并返回重复后的张量。默认为<span class="token boolean">None</span>。<span class="token comment"># pos = pos.repeat_interleave(repeats, dim=None)</span><span class="token comment"># pos = torch. repeat_interleave(input, repeats, dim)</span><span class="token operator">>></span><span class="token operator">></span> x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> x<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 传入多维张量，默认`flatten`</span><span class="token operator">>></span><span class="token operator">></span> y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 指定维度</span><span class="token operator">>></span><span class="token operator">></span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>y<span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 指定不同元素重复不同次数</span><span class="token operator">>></span><span class="token operator">></span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>y<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>y<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id="torch-nn-Flatten-start-dim-x3D-1-end-dim-x3D-1"><a href="#torch-nn-Flatten-start-dim-x3D-1-end-dim-x3D-1" class="headerlink" title="torch.nn.Flatten(start_dim&#x3D;1, end_dim&#x3D;-1)"></a>torch.nn.Flatten(start_dim&#x3D;1, end_dim&#x3D;-1)</h3><p>注意默认开始的维度是第一维！</p><p><img src="/2022/07/02/summary-of-tensor-function-in-pytorch/nndeflattenshishenmns.png" alt="torch.nn.Flatten(start_dim=1, end_dim=-1)"></p><pre class="language-python" data-language="python"><code class="language-python"><span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token comment"># With default parameters</span>m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>output <span class="token operator">=</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># torch.Size([32, 25])</span><span class="token comment"># With non-default parameters</span>m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>output <span class="token operator">=</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># torch.Size([160, 5])</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tensor </tag>
            
            <tag> function in Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is Normalization</title>
      <link href="/2022/06/10/what-is-normalization/"/>
      <url>/2022/06/10/what-is-normalization/</url>
      
        <content type="html"><![CDATA[<h2 id="What-is-Normalization"><a href="#What-is-Normalization" class="headerlink" title="What is Normalization?"></a>What is Normalization?</h2><h3 id="LayerNorm"><a href="#LayerNorm" class="headerlink" title="LayerNorm"></a><strong>LayerNorm</strong></h3><p><img src="/2022/06/10/what-is-normalization/layernorm.jpg"></p><p>在transformer中一般采用LayerNorm，LayerNorm也是归一化的一种方法，<strong>与BatchNorm不同的是它是对每单个batch进行的归一化，而batchnorm是对所有batch一起进行归一化的</strong>。</p><h4 id="normalized-shape-x3D-4"><a href="#normalized-shape-x3D-4" class="headerlink" title="normalized_shape &#x3D; 4"></a>normalized_shape &#x3D; 4</h4><p><strong>如果normalized_shape &#x3D; 4，则被看做一个整数的list</strong>，此时LayerNorm会对输入的最后一维进行归一化，normalized_shape &#x3D; 4需要和输入的最后一维一致。</p><p>比如此时输入的数据维度是[3, 4]，则对3个长度为4的向量求均值方差，得到3个均值和3个方差，分别对这3行进行归一化（每一行的4个数字都是均值为0，方差为1）；LayerNorm中的weight和bias也分别包含4个数字，重复使用3次，对每一行进行仿射变换（仿射变换即上述的运算公式，乘weight中对应的数字后，再加bias中对应的数字），并会在反向传播时得到学习。</p><h4 id="normalized-shape-x3D-3-4-or-torch-Size-3-4"><a href="#normalized-shape-x3D-3-4-or-torch-Size-3-4" class="headerlink" title="normalized_shape &#x3D; [3, 4] or torch.Size([3, 4])"></a>normalized_shape &#x3D; [3, 4] or torch.Size([3, 4])</h4><p>**如果输入的是个list或者torch.Size，比如[3, 4]或torch.Size([3, 4])**，则会对网络最后的两维进行归一化，且要求输入数据的最后两维尺寸也是[3, 4]。</p><p>假设此时输入的数据维度也是[3, 4]，首先对这12个数字求均值和方差，然后归一化这个12个数字；weight和bias也分别包含12个数字，分别对12个归一化后的数字进行仿射变换（仿射变换即乘以weight中对应的数字后，然后加bias中对应的数字），并会在反向传播时得到学习。</p><p>假设此时输入的数据维度是[N, 3, 4]，则对着N个[3,4]做和上述一样的操作，只是此时做仿射变换时，weight和bias被重复用了N次。</p><p>假设此时输入的数据维度是[N, T, 3, 4]，也是一样的，维度可以更多。</p><p><strong>注意：显然LayerNorm中weight和bias的shape就是传入的normalized_shape。</strong></p><h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a><strong>Batch Normalization</strong></h3><pre class="language-python" data-language="python"><code class="language-python">m <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm1d<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>   <span class="token comment">#num_features指的是randn(20, 100)中（N, C）的第二维C</span><span class="token builtin">input</span> <span class="token operator">=</span> autograd<span class="token punctuation">.</span>Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span> output <span class="token operator">=</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span> <span class="token comment"># Input: (N, C) or (N, C, L), where N is the batch size, C is the number of features or channels, and L is the sequence length</span><span class="token comment"># Output: (N, C) or (N, C, L) (same shape as input)</span></code></pre><pre class="language-python" data-language="python"><code class="language-python">m <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">35</span><span class="token punctuation">,</span> <span class="token number">45</span><span class="token punctuation">)</span>output <span class="token operator">=</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token comment"># Input: (N, C, H, W)</span><span class="token comment"># Output: (N, C, H, W) (same shape as input)</span></code></pre> <pre class="language-python" data-language="python"><code class="language-python">m <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm3d<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">35</span><span class="token punctuation">,</span> <span class="token number">45</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>output <span class="token operator">=</span> m<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token comment"># Input: (N, C, D, H, W)</span><span class="token comment"># Output: (N, C, D, H, W) (same shape as input)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LayerNorm </tag>
            
            <tag> BatchNorm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git Operation</title>
      <link href="/2022/06/04/git-operation/"/>
      <url>/2022/06/04/git-operation/</url>
      
        <content type="html"><![CDATA[<h2 id="How-to-build-a-repository-and-upload-files"><a href="#How-to-build-a-repository-and-upload-files" class="headerlink" title="How to build a repository and upload files?"></a>How to build a repository and upload files?</h2><h3 id="1-Build-a-new-repository"><a href="#1-Build-a-new-repository" class="headerlink" title="1.Build a new repository"></a>1.Build a new repository</h3><h3 id="2-Right-click-and-choose-‘git-bash-here’-in-Local-Project-Folder-clone-remote-repository"><a href="#2-Right-click-and-choose-‘git-bash-here’-in-Local-Project-Folder-clone-remote-repository" class="headerlink" title="2. Right-click and choose ‘git bash here’ in Local Project Folder, clone remote repository"></a>2. Right-click and choose ‘git bash here’ in Local Project Folder, clone remote repository</h3><p><code>git clone https://github.com/XXXXX/Your Project.git</code></p><h3 id="3-Local-Folder-Initialization"><a href="#3-Local-Folder-Initialization" class="headerlink" title="3.Local Folder Initialization"></a>3.Local Folder Initialization</h3><p><code>cd Your Project</code><br><code>git init </code></p><h3 id="5-Add-files-to-Local-Folder"><a href="#5-Add-files-to-Local-Folder" class="headerlink" title="5.Add files to Local Folder"></a>5.Add files to Local Folder</h3><p><code>git add .</code></p><h3 id="6-Commit-code-with-annotation"><a href="#6-Commit-code-with-annotation" class="headerlink" title="6.Commit code with annotation"></a>6.Commit code with annotation</h3><p>Annotations, like ‘initial commit’ are indispensable.</p><p><code>git commit -m &#39;initial commit&#39;</code></p><h3 id="7-push-code-to-remote-repository"><a href="#7-push-code-to-remote-repository" class="headerlink" title="7.push code to remote repository"></a>7.push code to remote repository</h3><p><code>git push</code></p>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git operation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>One hot Encoding and Sparse Vector</title>
      <link href="/2022/06/03/one-hot-encoding-and-sparse-vector/"/>
      <url>/2022/06/03/one-hot-encoding-and-sparse-vector/</url>
      
        <content type="html"><![CDATA[<h2 id="one-hot-vectorial-representations"><a href="#one-hot-vectorial-representations" class="headerlink" title="one-hot vectorial representations"></a><strong>one-hot vectorial representations</strong></h2><p>独热编码，即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。</p><p>例如对六个状态进行编码：</p><p>自然顺序码为 000,001,010,011,100,101</p><p>独热编码则是 000001,000010,000100,001000,010000,100000</p><h2 id="Sparse-Vector-versus-Dense-Vector"><a href="#Sparse-Vector-versus-Dense-Vector" class="headerlink" title="Sparse Vector versus Dense Vector"></a><strong>Sparse Vector versus Dense Vector</strong></h2><p>Vector(1.0,0.0,1.0,3.0) </p><p>Dense Vector[1.0,0.0,1.0,3.0]</p><p>Sparse Vector(4,[0,2,3],[1.0,1.0,3.0])</p><p>第一个4表示向量的长度(元素个数)，[0,2,3]就是indices数组；[1.0,1.0,3.0]是values数组，表示向量0的位置的值是1.0，2的位置的值是1.0,而3的位置的值是3.0,其他的位置都是0。</p><p>稀疏向量在面对很多数据是零的情况下能节省很多的存储空间</p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> one-hot vectorial </tag>
            
            <tag> Sparse Vector </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>From RNN to LSTM</title>
      <link href="/2022/06/01/rnn-and-lstm/"/>
      <url>/2022/06/01/rnn-and-lstm/</url>
      
        <content type="html"><![CDATA[<h2 id="RNN-and-LSTM"><a href="#RNN-and-LSTM" class="headerlink" title="RNN and LSTM"></a>RNN and LSTM</h2><h3 id="nn-embedding"><a href="#nn-embedding" class="headerlink" title="nn.embedding"></a><strong>nn.embedding</strong></h3><p>nn.embedding 构造一个vocab size的lookup table，每个vocabulary用hidden_size维度的向量表示</p> <pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> embedding<span class="token punctuation">.</span>weight<span class="token comment"># Parameter containing:          </span><span class="token comment"># tensor([[ 1.2402, -1.0914, -0.5382],</span><span class="token comment">#         [-1.1031, -1.2430, -0.2571],</span><span class="token comment">#         [ 1.6682, -0.8926, 1.4263],</span><span class="token comment">#         [ 0.8971, 1.4592, 0.6712],</span><span class="token comment">#         [-1.1625, -0.1598, 0.4034],</span><span class="token comment">#         [-0.2902, -0.0323, -2.2259],</span><span class="token comment">#         [ 0.8332, -0.2452, -1.1508],</span><span class="token comment">#         [ 0.3786, 1.7752, -0.0591],</span><span class="token comment">#         [-1.8527, -2.5141, -0.4990],</span><span class="token comment">#         [-0.6188, 0.5902, -0.0860]], requires_grad=True)</span><span class="token operator">>></span><span class="token operator">></span> embedding<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>size<span class="token comment"># torch.Size([10, 3])</span></code></pre><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># note that input is indices, the size of which is [2,4]</span><span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>a <span class="token operator">=</span> embedding<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>   <span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment"># tensor([[[-1.1031, -1.2430, -0.2571],   </span><span class="token comment">#          [ 1.6682, -0.8926, 1.4263],  </span><span class="token comment">#          [-1.1625, -0.1598, 0.4034],</span><span class="token comment">#          [-0.2902, -0.0323, -2.2259]],</span><span class="token comment">#         [[-1.1625, -0.1598, 0.4034],</span><span class="token comment">#          [ 0.8971, 1.4592, 0.6712],</span><span class="token comment">#          [ 1.6682, -0.8926, 1.4263],</span><span class="token comment">#          [-0.6188, 0.5902, -0.0860]]], grad_fn=&lt;EmbeddingBackward>)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># torch.Size([2, 4, 3])</span></code></pre><p>a &#x3D; embedding(input)是去embedding.weight中取对应index的词向量！</p><p>看a的第一行，input处index&#x3D;1，对应取出weight中index&#x3D;1的值，即按index取词向量！</p><h3 id="Vanilla-RNN"><a href="#Vanilla-RNN" class="headerlink" title="Vanilla RNN"></a><strong>Vanilla RNN</strong></h3><p><img src="/2022/06/01/rnn-and-lstm/rnn.jpg" alt="RNN"></p><pre class="language-python" data-language="python"><code class="language-python">bs <span class="token operator">=</span> <span class="token number">20</span>vocab_size <span class="token operator">=</span><span class="token number">10000</span><span class="token keyword">class</span> <span class="token class-name">three_layer_recurrent_net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>three_layer_recurrent_net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span> vocab_size  <span class="token punctuation">,</span> hidden_size <span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span>       hidden_size <span class="token punctuation">,</span> hidden_size <span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>    hidden_size <span class="token punctuation">,</span> vocab_size  <span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> word_seq<span class="token punctuation">,</span> h_init <span class="token punctuation">)</span><span class="token punctuation">:</span>g_seq <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span> word_seq <span class="token punctuation">)</span> h_seq <span class="token punctuation">,</span> h_final  <span class="token operator">=</span>  self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span> g_seq <span class="token punctuation">,</span> h_init <span class="token punctuation">)</span>score_seq <span class="token operator">=</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">(</span> h_seq <span class="token punctuation">)</span><span class="token keyword">return</span> score_seq<span class="token punctuation">,</span> h_final</code></pre><p>nn.RNN(<strong>input_size, hidden_size</strong>, num_layers&#x3D;1, nonlinearity&#x3D;tanh, bias&#x3D;True, batch_first&#x3D;False, dropout&#x3D;0, bidirectional&#x3D;False)</p><p>input_size输入特征的维度， 一般rnn中输入的是词向量，那么 input_size 就等于一个词向量的维度（nn.Embedding()里定义了词向量维度）,或词向量的feature。</p><p>hidden_size隐藏层神经元个数，或者也叫输出的维度（因为rnn输出为各个时间步上的隐藏状态），<strong>可以不等于</strong>input_size，网络中会自动进行升降维度。</p><p>num_layers: Number of <strong>recurrent layers</strong>. Setting num_layers&#x3D;2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1</p><p>nonlinearity激活函数，只能用relu或tanh（默认）</p><p>bias是否使用偏置</p><p>batch_first输入数据的形式</p><p>dropout是否在每个<strong>recurrent layer</strong>(RNN layer)后应用dropout（除最后一层）,默认不使用，如若使用将其设置成一个0-1的数字即可</p><p>birdirectional是否使用双向的 rnn，默认是 False，双向RNN对输入输出维度会有影响</p><p># 输入</p><p>input_shape为[seq_length, batch_size, hidden_size]</p><p>在例子里，g_seq是size为[word_seq, hidden_size]的张量，或者可以写成[seq_length, batch_size, hidden_size]</p><p>h_init的shape为[num_layers, batch_size, hidden_size]</p><p># 输出</p><p>在前向计算后会分别返回<strong>输出output和隐藏状态h_n</strong>。其中输出指的是隐藏层在各个时间步上计算并输出的隐藏状态，它们通常作为后续输出层的输⼊。需要强调的是，该“输出”本身并不涉及输出层计算，size为[seq_length, batch_size, hidden_size]；隐藏状态指的是隐藏层在最后时间步的隐藏状态：当隐藏层有多层时，每⼀层的隐藏状态都会记录在该变量中，隐藏状态h的size为[num_layers, batch_size, hidden_size]</p><p><img src="/2022/06/01/rnn-and-lstm/rnn_h_t.jpg"></p><pre class="language-python" data-language="python"><code class="language-python">input_size <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">(</span>word vector dimension <span class="token keyword">or</span> <span class="token builtin">input</span> feature<span class="token punctuation">)</span><span class="token punctuation">,</span> hidden_size <span class="token operator">=</span> <span class="token number">6</span><span class="token punctuation">,</span> num_layers <span class="token operator">=</span> <span class="token number">2</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">21</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token comment"># seq_length = 21, (21 words, 21 time_steps), batch_size = 8, (8 sentences), every single word vector’s size = 5</span><span class="token comment"># 假设一个batch里面有8个句子，现在走到第17个time step，同时计算的是这个batch里面，8个不同sequence中第17个词，得到这8个词的状态之后，传到下一个timestep对应的cell，再计算8个不同sequence中第18个词 </span><span class="token comment"># num_layer = 7, batch_size = 8, hidden_size = 6</span>h0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span>output<span class="token punctuation">,</span> hn <span class="token operator">=</span> rnn<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> h0<span class="token punctuation">)</span><span class="token comment"># output.size() = torch.Size([21, 8, 6])</span><span class="token comment"># hn.size() = torch.Size([7, 8, 6])</span></code></pre><h3 id="RNN-training-in-10-epoch"><a href="#RNN-training-in-10-epoch" class="headerlink" title="RNN training in 10 epoch"></a><strong>RNN training in 10 epoch</strong></h3><pre class="language-python" data-language="python"><code class="language-python">hidden_size<span class="token operator">=</span><span class="token number">150</span>net <span class="token operator">=</span> three_layer_recurrent_net<span class="token punctuation">(</span> hidden_size <span class="token punctuation">)</span>criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>my_lr <span class="token operator">=</span> <span class="token number">1</span>seq_length <span class="token operator">=</span> <span class="token number">35</span>start<span class="token operator">=</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># keep the learning rate to 1 during the first 4 epochs, then divide by 1.1 at every epoch</span><span class="token keyword">if</span> epoch <span class="token operator">>=</span> <span class="token number">4</span><span class="token punctuation">:</span>my_lr <span class="token operator">=</span> my_lr <span class="token operator">/</span> <span class="token number">1.1</span><span class="token comment"># create a new optimizer and give the current learning rate.  </span>optimizer<span class="token operator">=</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">,</span> lr<span class="token operator">=</span>my_lr <span class="token punctuation">)</span><span class="token comment"># set the running quantities to zero at the beginning of the epoch</span>    running_loss<span class="token operator">=</span><span class="token number">0</span>    num_batches<span class="token operator">=</span><span class="token number">0</span>      <span class="token comment"># set the initial h to be the zero vector</span>    h <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> bs<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>    <span class="token comment"># send it to the gpu  </span>    h<span class="token operator">=</span>h<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>    <span class="token keyword">for</span> count <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span> <span class="token number">0</span> <span class="token punctuation">,</span> <span class="token number">46478</span><span class="token operator">-</span>seq_length <span class="token punctuation">,</span>  seq_length<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># Set the gradients to zeros</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># create a minibatch, every unit output should be compared with the next step, </span>        <span class="token comment"># so minibatch_label should be [ count+1 : count+seq_length+1 ]</span>        minibatch_data <span class="token operator">=</span> train_data<span class="token punctuation">[</span>  count  <span class="token punctuation">:</span>  count<span class="token operator">+</span>seq_length  <span class="token punctuation">]</span>        minibatch_label <span class="token operator">=</span> train_data<span class="token punctuation">[</span> count<span class="token operator">+</span><span class="token number">1</span> <span class="token punctuation">:</span> count<span class="token operator">+</span>seq_length<span class="token operator">+</span><span class="token number">1</span> <span class="token punctuation">]</span>            <span class="token comment"># send them to the gpu</span>        minibatch_data<span class="token operator">=</span>minibatch_data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        minibatch_label<span class="token operator">=</span>minibatch_label<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        <span class="token comment"># Detach to prevent from backpropagating all the way to the beginning</span>        <span class="token comment"># Then tell Pytorch to start tracking all operations that will be done on h</span>        h<span class="token operator">=</span>h<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>        h<span class="token operator">=</span>h<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># forward the minibatch through the net    </span>        scores<span class="token punctuation">,</span> h <span class="token operator">=</span> net<span class="token punctuation">(</span> minibatch_data<span class="token punctuation">,</span> h <span class="token punctuation">)</span>        <span class="token comment"># reshape the scores and labels to huge batch of size bs*seq_length</span>        scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>view<span class="token punctuation">(</span> bs<span class="token operator">*</span>seq_length <span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span>         minibatch_label <span class="token operator">=</span>  minibatch_label<span class="token punctuation">.</span>view<span class="token punctuation">(</span> bs<span class="token operator">*</span>seq_length <span class="token punctuation">)</span>            <span class="token comment"># Compute the average of the losses of the data points in this huge batch</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span> scores <span class="token punctuation">,</span> minibatch_label <span class="token punctuation">)</span>        <span class="token comment"># backward pass to compute dL/dR, dL/dV and dL/dW</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># do one step of stochastic gradient descent: R=R-lr(dL/dR), V=V-lr(dL/dV), ...</span>        utils<span class="token punctuation">.</span>normalize_gradient<span class="token punctuation">(</span>net<span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># update the running loss </span>        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        num_batches <span class="token operator">+=</span> <span class="token number">1</span> <span class="token comment"># compute stats for the full training set</span>total_loss <span class="token operator">=</span> running_loss<span class="token operator">/</span>num_batcheselapsed <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-</span>start<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch='</span><span class="token punctuation">,</span>epoch<span class="token punctuation">,</span> <span class="token string">'\t time='</span><span class="token punctuation">,</span> elapsed<span class="token punctuation">,</span><span class="token string">'\t lr='</span><span class="token punctuation">,</span> my_lr<span class="token punctuation">,</span> <span class="token string">'\t exp(loss)='</span><span class="token punctuation">,</span> math<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>total_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>eval_on_test_set<span class="token punctuation">(</span><span class="token punctuation">)</span> </code></pre><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a><strong>LSTM</strong></h3><p>help alleviate the problem of vanishing and exploding gradients in RNN</p><p>长短期记忆（LSTM），隐藏状态是⼀个元组(h, c)，即hidden state和cell state</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">three_layer_recurrent_net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token builtin">super</span><span class="token punctuation">(</span>three_layer_recurrent_net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span> vocab_size <span class="token punctuation">,</span> hidden_size <span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>   hidden_size <span class="token punctuation">,</span> hidden_size <span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>  hidden_size <span class="token punctuation">,</span> vocab_size  <span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> word_seq<span class="token punctuation">,</span> h_init<span class="token punctuation">,</span> c_init <span class="token punctuation">)</span><span class="token punctuation">:</span>g_seq <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>word_seq <span class="token punctuation">)</span> h_seq<span class="token punctuation">,</span><span class="token punctuation">(</span>h_final<span class="token punctuation">,</span>c_final<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span> g_seq <span class="token punctuation">,</span> <span class="token punctuation">(</span>h_init<span class="token punctuation">,</span>c_init<span class="token punctuation">)</span> <span class="token punctuation">)</span>score_seq <span class="token operator">=</span> self<span class="token punctuation">.</span>layer3<span class="token punctuation">(</span>h_seq<span class="token punctuation">)</span><span class="token keyword">return</span> score_seq<span class="token punctuation">,</span> h_final <span class="token punctuation">,</span> c_finallstm <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>h0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>c0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>output<span class="token punctuation">,</span> <span class="token punctuation">(</span>hn<span class="token punctuation">,</span> cn<span class="token punctuation">)</span> <span class="token operator">=</span> lstm<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>h0<span class="token punctuation">,</span> c0<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> Embedding </tag>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LC Dynamic Programming</title>
      <link href="/2022/05/31/lc-dynamic-programming/"/>
      <url>/2022/05/31/lc-dynamic-programming/</url>
      
        <content type="html"><![CDATA[<h2 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming"></a><strong>Dynamic Programming</strong></h2><h3 id="打家劫舍问题"><a href="#打家劫舍问题" class="headerlink" title="打家劫舍问题"></a>打家劫舍问题</h3><p>偷了 <strong>i</strong> 家，则 <strong>i-1</strong> 家和 <strong>i+1</strong> 家都不能偷</p><p>nums列表存储的是第 <strong>i</strong> 家中的金币数量</p><p>dp列表存储的是前 <strong>i</strong> 家时小偷能够获得的最大金币数</p><pre class="language-python" data-language="python"><code class="language-python">dp <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> sizedp<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>dp<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>nums<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> nums<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>dp<span class="token punctuation">[</span>i <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> dp<span class="token punctuation">[</span>i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id="最大正方形"><a href="#最大正方形" class="headerlink" title="最大正方形"></a>最大正方形</h3><p>matrix存储元素为0或1字符的一个m x n矩阵，要求其中1元素能组成的最大正方形的边长</p><p>dp列表存储的第<strong>i</strong>行第<strong>j</strong>列可能构成的最大正方形边长</p><p>如果(i,j)处元素为1，则此处能构成的最大正方形边长为(i-1,j) (i,j-1) (i-1,j-1)中能构成的最大边长的最小值+1</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> j <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>matrix<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">elif</span> i <span class="token operator">!=</span> <span class="token number">0</span> <span class="token keyword">and</span> j <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token keyword">if</span> matrix<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'1'</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token keyword">else</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">if</span> dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> <span class="token builtin">max</span><span class="token punctuation">:</span><span class="token builtin">max</span> <span class="token operator">=</span> dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span></code></pre><h3 id="最长公共子序列-LCS"><a href="#最长公共子序列-LCS" class="headerlink" title="最长公共子序列(LCS)"></a>最长公共子序列(LCS)</h3><p>设X&#x3D;x1x2…xm和Y&#x3D;y1y2…yn是两个序列，Z&#x3D;z1z2…zk是这两个序列的一个最长公共子序列。</p><ol><li><p>如果xm&#x3D;yn，那么zk&#x3D;xm&#x3D;yn，且Zk-1是Xm-1，Yn-1的一个最长公共子序列；</p></li><li><p>如果xm≠yn，那么zk≠xm，意味着Z是Xm-1，Y的一个最长公共子序列；</p></li><li><p>如果xm≠yn，那么zk≠yn，意味着Z是X，Yn-1的一个最长公共子序列。</p></li></ol><p>我们使用dp[i][j]来表示第一个串的前i位和第二个串的前j位中的最长公共子序列，我们很容易能发现当两个串的任意一个串的当前长度为0时，它的最长公共子序列的长度为0，所以先对dp数组的边界进行初始化。然后我们发现，如果a[i]&#x3D;b[j]，dp[i][j]&#x3D;dp[i-1][j-1]+1，很显然，当比对的位字符一样时，能得到该状态转移方程。如果a[i]≠b[j]，dp[i][j]&#x3D;max(dp[i-1][j],dp[i][j-1])，该状态转移方程是由上面的2，3条取最大值得到的。</p><pre class="language-python" data-language="python"><code class="language-python">l1 <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text1<span class="token punctuation">)</span>l2 <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text2<span class="token punctuation">)</span>dp <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token punctuation">(</span>l2<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>l1 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> l1 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> l2 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">if</span> text1<span class="token punctuation">[</span>i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">!=</span> text2<span class="token punctuation">[</span>j <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span> dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>p<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token keyword">return</span> dp<span class="token punctuation">[</span>l1<span class="token punctuation">]</span><span class="token punctuation">[</span>l2<span class="token punctuation">]</span> </code></pre><h3 id="最长公共子串"><a href="#最长公共子串" class="headerlink" title="最长公共子串"></a>最长公共子串</h3><p>我们使用dp[i][j]来表示第一个串的前i位和第二个串的前j位中的最长公共子串，我们很容易能发现当两个串的任意一个串的当前长度为0时，它的最长公共子序列的长度为0，所以先对dp数组的边界进行初始化。然后我们发现，如果a[i]&#x3D;b[j]，dp[i][j]&#x3D;dp[i-1][j-1]+1，很显然，当比对的位字符一样时，能得到该状态转移方程。如果a[i]≠b[j]，dp[i][j]&#x3D;0，说明无论之前有没有连续的子串，到了这个不相等的位置会直接断掉，所以dp[i][j]&#x3D;0;</p><pre class="language-python" data-language="python"><code class="language-python">dp <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token punctuation">(</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>m<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token builtin">max</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>m<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">if</span> s1<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> s2<span class="token punctuation">[</span>j<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token keyword">else</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">if</span> dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">></span><span class="token builtin">max</span><span class="token punctuation">:</span><span class="token builtin">max</span> <span class="token operator">=</span> dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">max</span><span class="token punctuation">)</span></code></pre><h3 id="有效括号组合"><a href="#有效括号组合" class="headerlink" title="有效括号组合"></a>有效括号组合</h3><p>数字 n 代表生成括号的对数，设计一个函数，用于能够生成所有可能的并且<strong>有效的</strong>括号组合。</p><p><strong>固定第n个括号的左括号在最左，则n-1个括号可以在n括号内或外</strong>，通过动态规划可以直到小于n时的任意个括号的有效组合</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> n <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>total <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>total<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">)</span>total<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'()'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>s <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>now_list1 <span class="token operator">=</span> total<span class="token punctuation">[</span>j<span class="token punctuation">]</span>        <span class="token comment"># p = j 时的括号组合情况</span>now_list2 <span class="token operator">=</span> total<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token operator">-</span>j<span class="token punctuation">]</span>    <span class="token comment"># q = (i-1) - j 时的括号组合情况,因为已经固定了一个</span><span class="token keyword">for</span> k1 <span class="token keyword">in</span> now_list1<span class="token punctuation">:</span><span class="token keyword">for</span> k2 <span class="token keyword">in</span> now_list2<span class="token punctuation">:</span><span class="token keyword">if</span> k1 <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>k1 <span class="token operator">=</span> <span class="token string">''</span><span class="token keyword">if</span> k2 <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>k2 <span class="token operator">=</span> <span class="token string">''</span>en <span class="token operator">=</span> <span class="token string">'('</span> <span class="token operator">+</span> k1 <span class="token operator">+</span> <span class="token string">')'</span> <span class="token operator">+</span> k2s<span class="token punctuation">.</span>append<span class="token punctuation">(</span>en<span class="token punctuation">)</span>total<span class="token punctuation">.</span>append<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token keyword">return</span> total<span class="token punctuation">[</span>n<span class="token punctuation">]</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Dynamic Programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LC Slicing of List</title>
      <link href="/2022/05/31/lc-slicing-of-list/"/>
      <url>/2022/05/31/lc-slicing-of-list/</url>
      
        <content type="html"><![CDATA[<h2 id="Slicing-of-List"><a href="#Slicing-of-List" class="headerlink" title="Slicing of List"></a><strong>Slicing of List</strong></h2><pre class="language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token comment"># 当索引只有一个数时，表示切取某一个元素。</span><span class="token operator">>></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span><span class="token number">0</span><span class="token operator">>></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span><span class="token number">6</span><span class="token comment"># 从左往右</span><span class="token operator">>></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token comment"># 从左往右</span><span class="token operator">>></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token comment"># 从右往左</span><span class="token operator">>></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token comment"># step=1，从左往右取值，start_index=1到end_index=6同样表示从左往右取值。</span><span class="token operator">>></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment"># 输出为空列表，说明没取到数据。</span><span class="token comment"># step=-1，决定了从右往左取值，而start_index=1到end_index=6决定了从左往右取值，两者矛盾，所以为空。</span><span class="token operator">>></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token comment"># 同样输出为空列表。</span><span class="token comment"># step=1，决定了从左往右取值，而start_index=6到end_index=2决定了从右往左取值，两者矛盾，所以为空。</span><span class="token operator">>></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token comment"># step=1，表示从左往右取值，而start_index省略时，表示从端点开始，因此这里的端点是“起点”，即从“起点”值0开始一直取到end_index=6（该点不包括）。</span><span class="token operator">>></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token comment"># step=-1，从右往左取值，而start_index省略时，表示从端点开始，因此这里的端点是“终点”，即从“终点”值9开始一直取到end_index=6（该点不包括）。</span><span class="token operator">>></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token comment"># step=1，从左往右取值，从start_index=6开始，一直取到“终点”值9。</span><span class="token operator">>></span><span class="token operator">></span>a<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">>></span><span class="token operator">></span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token comment"># step=-1，从右往左取值，从start_index=6开始，一直取到“起点”0。</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DNN Gradient Vanishing or Exploding</title>
      <link href="/2022/05/30/dnn-gradient-vanishing-or-exploding/"/>
      <url>/2022/05/30/dnn-gradient-vanishing-or-exploding/</url>
      
        <content type="html"><![CDATA[<h2 id="DNN-Gradient-Vanishing-or-Exploding"><a href="#DNN-Gradient-Vanishing-or-Exploding" class="headerlink" title="DNN Gradient Vanishing or Exploding"></a>DNN Gradient Vanishing or Exploding</h2><p>build 2 hidden layer DNN with an output layer</p><p><img src="/2022/05/30/dnn-gradient-vanishing-or-exploding/vanish_1.png"></p><p>loss is function of f3</p><p><img src="/2022/05/30/dnn-gradient-vanishing-or-exploding/vanish_2.png"></p><p>根据上面规律，我们可以把x写成f0，当有n-1层隐层时，fn是输出，如果要求wl也就是第l层的权重，反向传播中涉及的偏导计算为：</p><p><img src="/2022/05/30/dnn-gradient-vanishing-or-exploding/vanish_3.png"></p><p>前半部分是关于激活函数的导数的累乘，后半部分是关于权重值的累乘。</p><p>激活函数如sigmoid函数，其导数的取值范围是(0, 0.25]，当网络层数很深的时候，多个小于1的数进行累乘，结果是趋向于0的，此时梯度反向传播的时候，根据参数更新公式：$$w <em>{l} &#x3D; w</em>{l}-lr·\frac{\partial loss}{\partial w _{l}} $$ ，偏导部分的取值趋于0，那么该参数得不到更新，这就是梯度消失现象。消失的只是更新参数所需要的梯度，而不一定是参数被更新到0。</p><p>关于权重值的累乘，当我们初始化权值很大的时候，多个大于1的数累乘，结果是+∞，此时就出现了梯度爆炸现象。</p><h2 id="NN-detach"><a href="#NN-detach" class="headerlink" title="NN: detach()"></a><strong>NN: detach()</strong></h2><p>detach()是生成了一个新的variable，detach_()是对本身的更改，</p><p>detach()</p><p>（1）返回一个<strong>新的从当前图中分离</strong>的Variable。</p><p>（2）返回的 Variable 不会梯度更新</p><p>（3）被detach()的相当于**with torch.no_grad()**操作</p><p>（4）返回的Variable和被detach的Variable指向同一个tensor</p><pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">2</span><span class="token operator">>></span><span class="token operator">></span> y<span class="token punctuation">.</span>requires_grad<span class="token boolean">False</span></code></pre><h2 id="NN-detach-1"><a href="#NN-detach-1" class="headerlink" title="NN: detach_()"></a><strong>NN: detach_()</strong></h2><p>Detach_()</p><p>（1）将<strong>一个Variable</strong>从创建它的图中分离，并把它设置成<strong>叶子Variable</strong>。</p><p>（2）将中间节点的grad_fn的值设置为None时，这样中间节点就不会再与前一个节点关联，此时的中间节点就变成了叶子节点。</p><p>（3）将中间节点的requires_grad设置为False，这样对后面节点进行backward()时就不会对中间节点求梯度</p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DNN </tag>
            
            <tag> Gradient </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dropout? Dropwhat?</title>
      <link href="/2022/05/29/dropout/"/>
      <url>/2022/05/29/dropout/</url>
      
        <content type="html"><![CDATA[<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a><strong>Dropout</strong></h2><p>一般出现在<strong>全连接层</strong>后，防止模型过拟合。</p><p>神经网络的输入单元是否归零服从<strong>伯努利分布</strong>，并以概率p随机地将神经网络的某个单元的输出（对下一层而言是输入）置为0。</p><p>在训练中，每个隐层的神经元先乘以概率P，然后再进行激活。</p><p>在测试中，所有的神经元先进行激活，然后每个隐层神经元的输出乘P。</p><p><img src="/2022/05/29/dropout/dropout.jpg"></p> <pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">NeuralNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>NeuralNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>         self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>         self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token comment"># dropout训练</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        <span class="token keyword">return</span> out    model <span class="token operator">=</span> NeuralNet<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span></code></pre><h3 id="启用Batch-Normalization-和-Dropout"><a href="#启用Batch-Normalization-和-Dropout" class="headerlink" title="启用Batch Normalization 和 Dropout"></a>启用Batch Normalization 和 Dropout</h3><p>需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。</p><h3 id="不启用-Batch-Normalization-和-Dropout"><a href="#不启用-Batch-Normalization-和-Dropout" class="headerlink" title="不启用 Batch Normalization 和 Dropout"></a>不启用 Batch Normalization 和 Dropout</h3><p>如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout, model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。</p><p>训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的的性质。</p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dropout </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN Is All You Need</title>
      <link href="/2022/05/28/cnn/"/>
      <url>/2022/05/28/cnn/</url>
      
        <content type="html"><![CDATA[<h2 id="Convolutional-Layer"><a href="#Convolutional-Layer" class="headerlink" title="Convolutional Layer"></a><strong>Convolutional Layer</strong></h2><p>Make a convolutional module:</p> <pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">torch</span><span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span>                       stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># 前三个参数必填</span>mod <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span> <span class="token number">2</span> <span class="token punctuation">,</span> <span class="token number">5</span> <span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span></code></pre><p>inputs: 2 channels — every channel is a feature map, 灰度图片只有一个feature map；彩色图片一般3个feature map（RGB）</p><p>output: 5 activation maps — 5个激活映射，每个filter都会产生一个，所以有5个filters</p><p>filters are 3x3, padding with one layer of zero to not shrink anything</p><p><img src="/2022/05/28/cnn/howfilerswork.png" alt="How filters work?"></p><p><img src="/2022/05/28/cnn/depthwiseconv.png" alt="Depthwise Convolution"></p><h2 id="Receptive-Field"><a href="#Receptive-Field" class="headerlink" title="Receptive Field"></a><strong>Receptive Field</strong></h2><p>感受野（Receptive Field）的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。再通俗点的解释是，特征图上的一个点对应输入图上的区域，如图。</p><p><img src="/2022/05/28/cnn/receptivefield.jpg" alt="Receptive Field"></p><p>两个3*3卷积层的串联相当于1个5*5的卷积层，3个3*3的卷积层串联相当于1个7*7的卷积层，即<strong>3个3*3卷积层的感受野大小相当于1个7*7的卷积层</strong>。但是3个3*3的卷积层参数量只有<strong>7*7</strong>的一半左右，同时前者可以有<strong>3个</strong>非线性操作，而后者<strong>只有1个</strong>非线性操作，这样使得前者对于特征的学习能力更强。</p><h2 id="Effective-Receptive-Field"><a href="#Effective-Receptive-Field" class="headerlink" title="Effective Receptive Field"></a>Effective Receptive Field</h2><p>有效感受野(effective receptive field, ERF)，在卷积计算时，实际有效的感受野区域。在F0特征层中，特征点6可以描述其他所有特征点的部分信息，即图中交叠部分，特征点6代表的信息更“有效”。即越靠近感受野中心的区域越有效。在网络训练时，有效感受野对参数的影响更大。</p><p><img src="/2022/05/28/cnn/effectivereceptivefield.png" alt="Effective Receptive Field"></p><p>一般而言，特征点有效感受野要小于实际感受野。其有效性，类高斯分布向边缘递减，且不同的激活函数对有效感受野影响不同</p><h2 id="Pooling-Layer"><a href="#Pooling-Layer" class="headerlink" title="Pooling Layer"></a><strong>Pooling Layer</strong></h2><p>Make a pooling module</p><p>inputs: activation maps of size n x n</p><p>output: activation maps of size n&#x2F;p x n&#x2F;p</p><p>p: pooling size, in this case p &#x3D;&#x3D; 2</p> <pre class="language-python" data-language="python"><code class="language-python">mod <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>    <span class="token comment"># max pooling</span><span class="token comment"># block 1: 3 x 32 x 32 --> 64 x 16 x 16    </span>self<span class="token punctuation">.</span>conv1a <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>self<span class="token punctuation">.</span>conv1b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>self<span class="token punctuation">.</span>pool1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span></code></pre><h2 id="Unpooling-layer"><a href="#Unpooling-layer" class="headerlink" title="Unpooling layer"></a><strong>Unpooling layer</strong></h2><p><img src="/2022/05/28/cnn/BedofNails.jpg" alt="Bed of Nails"></p><p><img src="/2022/05/28/cnn/NearestNeighbor.jpg" alt="Nearest Neighbor"></p><p><img src="/2022/05/28/cnn/MaxUnpooling.jpg" alt="Max Unpooling"></p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> Pooling and Unpooling </tag>
            
            <tag> Receptive Field </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vanilla Neural Networks</title>
      <link href="/2022/05/28/vanilla-neural-networks/"/>
      <url>/2022/05/28/vanilla-neural-networks/</url>
      
        <content type="html"><![CDATA[<h2 id="torch-softmax-x2F-torch-nn-Softmax"><a href="#torch-softmax-x2F-torch-nn-Softmax" class="headerlink" title="torch.softmax() &#x2F; torch.nn.Softmax()"></a><strong>torch.softmax() &#x2F; torch.nn.Softmax()</strong></h2><p><img src="/2022/05/28/vanilla-neural-networks/1_softmax.jpg" alt="softmax"></p><pre class="language-python" data-language="python"><code class="language-python">A <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span> <span class="token punctuation">[</span> <span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">2</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token punctuation">,</span> <span class="token number">2.0</span> <span class="token punctuation">,</span> <span class="token number">1.5</span> <span class="token punctuation">]</span> <span class="token punctuation">,</span>  <span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">2</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token punctuation">,</span> <span class="token number">7.0</span> <span class="token punctuation">,</span> <span class="token number">1.5</span> <span class="token punctuation">]</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>p <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>A <span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>              <span class="token comment"># over rows</span><span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token comment"># tensor([[1.0730e-02, 4.8089e-02, 5.8585e-01, 3.5533e-01], </span><span class="token comment">#         [1.2282e-04, 5.5046e-04, 9.9526e-01, 4.0674e-03]])</span>p <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>A <span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># over columns</span><span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token comment"># tensor([[0.5000, 0.5000, 0.0067, 0.5000],</span><span class="token comment">#         [0.5000, 0.5000, 0.9933, 0.5000]])</span><span class="token comment"># torch.nn.LogSoftmax() is taking logarithms of the result of softmax(xi)</span><span class="token comment"># also we have torch.nn.Softmax()</span></code></pre><p>x is stored in one column, although it is represented in one line when typing it on the console.</p><pre class="language-python" data-language="python"><code class="language-python">x<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">2</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token punctuation">,</span> <span class="token number">2.0</span> <span class="token punctuation">,</span> <span class="token number">1.5</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>p<span class="token operator">=</span>torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear()"></a><strong>nn.Linear()</strong></h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># input of size  5 and output of size 3</span>mod <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>bias <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>         <span class="token comment"># bias = True by default</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">)</span><span class="token comment"># Linear(in_features=5, out_features=3, bias=True)</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">.</span>weight<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># Parameter containing:</span><span class="token comment"># tensor([[-0.0636, 0.1377, -0.1297, 0.4385, 0.1840],</span><span class="token comment"># [-0.4137, 0.2118, 0.2093, -0.0728, -0.2257],</span><span class="token comment"># [ 0.4318, -0.1557, 0.1055, 0.3528, 0.2025]], requires_grad=True)</span><span class="token comment"># torch.Size([3, 5])</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">.</span>bias<span class="token punctuation">)</span><span class="token comment"># Parameter containing:</span><span class="token comment"># tensor([ 0.1466, 0.2684, -0.0493], requires_grad=True)</span> <span class="token comment"># change the weight of mod</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    mod<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>    mod<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>     mod<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">.</span>weight<span class="token punctuation">)</span></code></pre><h2 id="vanilla-nn"><a href="#vanilla-nn" class="headerlink" title="vanilla nn"></a><strong>vanilla nn</strong></h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">two_layer_net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token builtin">super</span><span class="token punctuation">(</span>two_layer_net <span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span> input_size<span class="token punctuation">,</span> hidden_size <span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span> hidden_size<span class="token punctuation">,</span> output_size <span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>          <span class="token comment"># relu activation function</span>x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>p <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span>           <span class="token comment"># can be modified to meet your demands</span><span class="token keyword">return</span> p net <span class="token operator">=</span> two_layer_net<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token comment"># Alternatively, all the parameters of the network can be accessed # by **net.parameters()**.</span>list_of_param <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>list_of_param<span class="token punctuation">)</span><span class="token comment"># [Parameter containing:</span><span class="token comment">#tensor([[10.0000, 20.0000],</span><span class="token comment">#        [ 0.0500, -0.5119],</span><span class="token comment">#        [-0.1930, -0.1993],</span><span class="token comment">#        [-0.0208, -0.0490],</span><span class="token comment">#        [ 0.2011, -0.2519]], requires_grad=True),</span><span class="token comment"># Parameter containing:</span><span class="token comment"># tensor([ 0.1292, -0.3313, -0.3548, -0.5247, 0.1753], requires_grad=True), </span><span class="token comment"># Parameter containing:</span><span class="token comment"># tensor([[0.3178, -0.1838, -0.1930, -0.3816, 0.1850],</span><span class="token comment">#         [ 0.2342, -0.2743, 0.2424, -0.3598, 0.3090],</span><span class="token comment">#         [ 0.0876, -0.3785, 0.2032, -0.2937, 0.0382]], requires_grad=True), </span><span class="token comment">#Parameter containing:</span><span class="token comment"># tensor([ 0.2120, -0.2751, 0.2351], requires_grad=True)]</span></code></pre><h2 id="nn-NLLLoss"><a href="#nn-NLLLoss" class="headerlink" title="nn.NLLLoss()"></a><strong>nn.NLLLoss()</strong></h2><p>负对数似然损失函数(Negtive Log Likehood) </p><pre class="language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>nll <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>target1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>target2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>target3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>n1 <span class="token operator">=</span> nll<span class="token punctuation">(</span>a<span class="token punctuation">,</span>target1<span class="token punctuation">)</span><span class="token comment"># tensor(-1.)</span>n2 <span class="token operator">=</span> nll<span class="token punctuation">(</span>a<span class="token punctuation">,</span>target2<span class="token punctuation">)</span><span class="token comment"># tensor(-2.)</span>n3 <span class="token operator">=</span> nll<span class="token punctuation">(</span>a<span class="token punctuation">,</span>target3<span class="token punctuation">)</span><span class="token comment"># tensor(-3.)</span></code></pre><p>nn.NLLLoss()取出a中对应target位置的值并取负号，比如target[1]&#x3D;&#x3D;0，就取a中index&#x3D;0位置上的值再取负，作为NLLLoss的输出</p><h2 id="nn-CrossEntropyLoss"><a href="#nn-CrossEntropyLoss" class="headerlink" title="nn.CrossEntropyLoss()"></a><strong>nn.CrossEntropyLoss()</strong></h2><p><img src="/2022/05/28/vanilla-neural-networks/2_crossentropyloss.jpg" alt="Cross Entropy Loss"></p><pre class="language-python" data-language="python"><code class="language-python">mycrit<span class="token operator">=</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>labels<span class="token operator">=</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>scores<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span> <span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.7</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.3</span><span class="token punctuation">,</span> <span class="token number">5.0</span><span class="token punctuation">]</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>average_loss <span class="token operator">=</span> mycrit<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>labels<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'loss = '</span><span class="token punctuation">,</span> average_loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token comment"># loss = 0.023508397862315178</span></code></pre><h2 id="CrossEntropyLoss-x3D-Softmax-Log-NLL"><a href="#CrossEntropyLoss-x3D-Softmax-Log-NLL" class="headerlink" title="CrossEntropyLoss &#x3D; Softmax + Log + NLL"></a><strong>CrossEntropyLoss &#x3D; Softmax + Log + NLL</strong></h2><pre class="language-python" data-language="python"><code class="language-python">softmax_func<span class="token operator">=</span>nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>soft_output<span class="token operator">=</span>softmax_func<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>log_output<span class="token operator">=</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>soft_output<span class="token punctuation">)</span>nllloss_func<span class="token operator">=</span>nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>nllloss_output<span class="token operator">=</span>nllloss_func<span class="token punctuation">(</span>log_output<span class="token punctuation">,</span> y_target<span class="token punctuation">)</span></code></pre><h2 id="epoch"><a href="#epoch" class="headerlink" title="epoch"></a><strong>epoch</strong></h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment"># Do 15 passes through the training set</span>shuffled_indices<span class="token operator">=</span>torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">60000</span><span class="token punctuation">)</span><span class="token keyword">for</span> count <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">60000</span><span class="token punctuation">,</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># indices is a tensor, the following is slicing</span>indices<span class="token operator">=</span>shuffled_indices<span class="token punctuation">[</span>count<span class="token punctuation">:</span>count<span class="token operator">+</span>bs<span class="token punctuation">]</span>minibatch_data <span class="token operator">=</span> train_data<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>minibatch_label <span class="token operator">=</span> train_label<span class="token punctuation">[</span>indices<span class="token punctuation">]</span><span class="token comment"># pay attention on how to slice a tensor</span></code></pre><h2 id="epoch-monitoring-loss-time-lr-update"><a href="#epoch-monitoring-loss-time-lr-update" class="headerlink" title="epoch + monitoring loss + time + lr update"></a><strong>epoch + monitoring loss + time + lr update</strong></h2><pre class="language-python" data-language="python"><code class="language-python">start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>lr <span class="token operator">=</span> <span class="token number">0.05</span>   \# initial learning rate<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># learning rate strategy : divide the learning rate by 1.5 every 10 epochs</span><span class="token keyword">if</span> epoch<span class="token operator">%</span><span class="token number">10</span><span class="token operator">==</span><span class="token number">0</span> <span class="token keyword">and</span> epoch<span class="token operator">></span><span class="token number">10</span><span class="token punctuation">:</span> lr <span class="token operator">=</span> lr <span class="token operator">/</span> <span class="token number">1.5</span><span class="token comment"># create a new optimizer at the beginning of each epoch: give the current learning rate.</span>optimizer<span class="token operator">=</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">,</span> lr<span class="token operator">=</span>lr <span class="token punctuation">)</span>running_loss<span class="token operator">=</span><span class="token number">0</span>running_error<span class="token operator">=</span><span class="token number">0</span>num_batches<span class="token operator">=</span><span class="token number">0</span>shuffled_indices<span class="token operator">=</span>torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">60000</span><span class="token punctuation">)</span><span class="token keyword">for</span> count <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">60000</span><span class="token punctuation">,</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># Set the gradients to zeros</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># create a minibatch </span>        indices<span class="token operator">=</span>shuffled_indices<span class="token punctuation">[</span>count<span class="token punctuation">:</span>count<span class="token operator">+</span>bs<span class="token punctuation">]</span>        minibatch_data <span class="token operator">=</span> train_data<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>        minibatch_label<span class="token operator">=</span> train_label<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>        <span class="token comment"># send them to the gpu</span>        device <span class="token operator">=</span> torch<span class="token punctuation">.</span> device<span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>        net <span class="token operator">=</span> net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        minibatch_data<span class="token operator">=</span>minibatch_data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        minibatch_label<span class="token operator">=</span>minibatch_label<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        <span class="token comment"># reshape the minibatch</span>        inputs <span class="token operator">=</span> minibatch_data<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span><span class="token number">784</span><span class="token punctuation">)</span>        <span class="token comment"># tell Pytorch to start tracking all operations that will be done on "inputs"</span>        inputs<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># forward the minibatch through the net </span>        scores<span class="token operator">=</span>net<span class="token punctuation">(</span> inputs <span class="token punctuation">)</span>         <span class="token comment"># Compute the average of the losses of the data points in the minibatch</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span> scores <span class="token punctuation">,</span> minibatch_label<span class="token punctuation">)</span>         <span class="token comment"># backward pass to compute dL/dU, dL/dV and dL/dW </span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># START COMPUTING STATS</span>        <span class="token comment"># add the loss of this batch to the running loss</span>        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># compute the error made on this batch and add it to the running error  </span>        error <span class="token operator">=</span> utils<span class="token punctuation">.</span>get_error<span class="token punctuation">(</span> scores<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">,</span> minibatch_label<span class="token punctuation">)</span>        running_error <span class="token operator">+=</span> error<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        num_batches<span class="token operator">+=</span><span class="token number">1</span>        <span class="token comment"># compute some stats</span>        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        error <span class="token operator">=</span> utils<span class="token punctuation">.</span>get_error<span class="token punctuation">(</span> scores<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> minibatch_label<span class="token punctuation">)</span>        running_error <span class="token operator">+=</span> error<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        num_batches<span class="token operator">+=</span><span class="token number">1</span>    <span class="token comment"># once the epoch is finished we divide the "running quantities"</span>    <span class="token comment"># by the number of batches</span>    total_loss <span class="token operator">=</span> running_loss<span class="token operator">/</span>num_batches    total_error <span class="token operator">=</span> running_error<span class="token operator">/</span>num_batches    elapsed_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> – start    <span class="token comment"># every 10 epoch we display the stats </span>    <span class="token comment"># and compute the error rate on the test set </span>    <span class="token keyword">if</span> epoch <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token punctuation">:</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch='</span><span class="token punctuation">,</span>epoch<span class="token punctuation">,</span> <span class="token string">' time='</span><span class="token punctuation">,</span> elapsed_time<span class="token punctuation">,</span><span class="token string">' loss='</span><span class="token punctuation">,</span> total_loss <span class="token punctuation">,</span> <span class="token string">' error='</span><span class="token punctuation">,</span> total_error<span class="token operator">*</span><span class="token number">100</span> <span class="token punctuation">,</span><span class="token string">'percent lr='</span><span class="token punctuation">,</span> lr<span class="token punctuation">)</span>eval_on_test_set<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="Evaluate-on-test-set"><a href="#Evaluate-on-test-set" class="headerlink" title="Evaluate on test set"></a><strong>Evaluate on test set</strong></h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">eval_on_test_set</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>running_error<span class="token operator">=</span><span class="token number">0</span>num_batches<span class="token operator">=</span><span class="token number">0</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">10000</span><span class="token punctuation">,</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>        minibatch_data <span class="token operator">=</span> test_data<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>bs<span class="token punctuation">]</span>        minibatch_label<span class="token operator">=</span> test_label<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>bs<span class="token punctuation">]</span>        inputs <span class="token operator">=</span> minibatch_data<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span><span class="token number">784</span><span class="token punctuation">)</span>        scores<span class="token operator">=</span>net<span class="token punctuation">(</span> inputs <span class="token punctuation">)</span>         error <span class="token operator">=</span> utils<span class="token punctuation">.</span>get_error<span class="token punctuation">(</span> scores <span class="token punctuation">,</span> minibatch_label<span class="token punctuation">)</span>        running_error <span class="token operator">+=</span> error<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        num_batches<span class="token operator">+=</span><span class="token number">1</span>    total_error <span class="token operator">=</span> running_error<span class="token operator">/</span>num_batches<span class="token keyword">print</span><span class="token punctuation">(</span> <span class="token string">'test error = '</span><span class="token punctuation">,</span> total_error<span class="token operator">*</span><span class="token number">100</span> <span class="token punctuation">,</span><span class="token string">'percent'</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Vanilla NN </tag>
            
            <tag> Softmax </tag>
            
            <tag> Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction of Tensors</title>
      <link href="/2022/05/27/introduction-of-tensors/"/>
      <url>/2022/05/27/introduction-of-tensors/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Reshaping-a-tensor"><a href="#1-Reshaping-a-tensor" class="headerlink" title="1.Reshaping a tensor"></a><strong>1.Reshaping a tensor</strong></h2><pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) </span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>          <span class="token comment"># two rows with five columns</span><span class="token comment"># tensor([[0, 1, 2, 3, 4],</span>          <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>Note that the original x is not modified, but p &#x3D; x.view(2,5) can store the change in variable p.</p><h2 id="2-Entries-of-a-tensor-can-be-scalar"><a href="#2-Entries-of-a-tensor-can-be-scalar" class="headerlink" title="2.Entries of a tensor can be scalar"></a><strong>2.Entries of a tensor can be scalar</strong></h2><p>A matrix is 2-dimensional Tensor</p><p>A row of a matrix is a 1-dimensional Tensor</p><p>An entry of a matrix is a 0-dimensional Tensor</p><p>0-dimensional Tensor are <strong>scalar</strong></p><p>If we want to convert a 0-dimensional Tensor into python number, we need to use <strong>item()</strong></p> <pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token comment"># how to slice a tensor: x[0], x[1:4], etc.</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># tensor(3.)</span><span class="token comment"># &lt;class 'torch.Tensor'></span><span class="token comment"># 3.0</span><span class="token comment"># &lt;class 'float'></span></code></pre><h2 id="3-The-Storage-of-Tensors"><a href="#3-The-Storage-of-Tensors" class="headerlink" title="3.The Storage of Tensors"></a><strong>3.The Storage of Tensors</strong></h2><pre class="language-python" data-language="python"><code class="language-python">A <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span> <span class="token comment"># tensor([0, 1, 2, 3, 4])</span>B <span class="token operator">=</span> A<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>storage<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data_ptr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>B<span class="token punctuation">.</span>storage<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data_ptr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 2076006947200</span><span class="token comment"># 2076006947200</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tensor </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
