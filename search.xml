<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>LC Dynamic Programming</title>
      <link href="/2022/05/31/lc-dynamic-programming/"/>
      <url>/2022/05/31/lc-dynamic-programming/</url>
      
        <content type="html"><![CDATA[<h2 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming"></a><strong>Dynamic Programming</strong></h2><h3 id="打家劫舍问题"><a href="#打家劫舍问题" class="headerlink" title="打家劫舍问题"></a>打家劫舍问题</h3><p>偷了 <strong>i</strong> 家，则 <strong>i-1</strong> 家和 <strong>i+1</strong> 家都不能偷</p><p>nums列表存储的是第 <strong>i</strong> 家中的金币数量</p><p>dp列表存储的是前 <strong>i</strong> 家时小偷能够获得的最大金币数</p><pre class="language-python" data-language="python"><code class="language-python">dp <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> sizedp<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> nums<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>dp<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>nums<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> nums<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>dp<span class="token punctuation">[</span>i <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+</span> nums<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> dp<span class="token punctuation">[</span>i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><h3 id="最大正方形"><a href="#最大正方形" class="headerlink" title="最大正方形"></a>最大正方形</h3><p>matrix存储元素为0或1字符的一个m x n矩阵，要求其中1元素能组成的最大正方形的边长</p><p>dp列表存储的第<strong>i</strong>行第<strong>j</strong>列可能构成的最大正方形边长</p><p>如果(i,j)处元素为1，则此处能构成的最大正方形边长为(i-1,j) (i,j-1) (i-1,j-1)中能构成的最大边长的最小值+1</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> j <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>matrix<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">elif</span> i <span class="token operator">!=</span> <span class="token number">0</span> <span class="token keyword">and</span> j <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token keyword">if</span> matrix<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">'1'</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token keyword">else</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">if</span> dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">></span> <span class="token builtin">max</span><span class="token punctuation">:</span><span class="token builtin">max</span> <span class="token operator">=</span> dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span></code></pre><h3 id="最长公共子序列-LCS"><a href="#最长公共子序列-LCS" class="headerlink" title="最长公共子序列(LCS)"></a>最长公共子序列(LCS)</h3><p>设X&#x3D;x1x2…xm和Y&#x3D;y1y2…yn是两个序列，Z&#x3D;z1z2…zk是这两个序列的一个最长公共子序列。</p><ol><li><p>如果xm&#x3D;yn，那么zk&#x3D;xm&#x3D;yn，且Zk-1是Xm-1，Yn-1的一个最长公共子序列；</p></li><li><p>如果xm≠yn，那么zk≠xm，意味着Z是Xm-1，Y的一个最长公共子序列；</p></li><li><p>如果xm≠yn，那么zk≠yn，意味着Z是X，Yn-1的一个最长公共子序列。</p></li></ol><p>我们使用dp[i][j]来表示第一个串的前i位和第二个串的前j位中的最长公共子序列，我们很容易能发现当两个串的任意一个串的当前长度为0时，它的最长公共子序列的长度为0，所以先对dp数组的边界进行初始化。然后我们发现，如果a[i]&#x3D;b[j]，dp[i][j]&#x3D;dp[i-1][j-1]+1，很显然，当比对的位字符一样时，能得到该状态转移方程。如果a[i]≠b[j]，dp[i][j]&#x3D;max(dp[i-1][j],dp[i][j-1])，该状态转移方程是由上面的2，3条取最大值得到的。</p><pre class="language-python" data-language="python"><code class="language-python">l1 <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text1<span class="token punctuation">)</span>l2 <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text2<span class="token punctuation">)</span>dp <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token punctuation">(</span>l2<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>l1 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> l1 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> l2 <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">if</span> text1<span class="token punctuation">[</span>i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">!=</span> text2<span class="token punctuation">[</span>j <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span>dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">,</span> dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">else</span><span class="token punctuation">:</span>p<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token keyword">return</span> dp<span class="token punctuation">[</span>l1<span class="token punctuation">]</span><span class="token punctuation">[</span>l2<span class="token punctuation">]</span> </code></pre><h3 id="最长公共子串"><a href="#最长公共子串" class="headerlink" title="最长公共子串"></a>最长公共子串</h3><p>我们使用dp[i][j]来表示第一个串的前i位和第二个串的前j位中的最长公共子串，我们很容易能发现当两个串的任意一个串的当前长度为0时，它的最长公共子序列的长度为0，所以先对dp数组的边界进行初始化。然后我们发现，如果a[i]&#x3D;b[j]，dp[i][j]&#x3D;dp[i-1][j-1]+1，很显然，当比对的位字符一样时，能得到该状态转移方程。如果a[i]≠b[j]，dp[i][j]&#x3D;0，说明无论之前有没有连续的子串，到了这个不相等的位置会直接断掉，所以dp[i][j]&#x3D;0;</p><pre class="language-python" data-language="python"><code class="language-python">dp <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token punctuation">(</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>m<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token builtin">max</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>m<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">if</span> s1<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> s2<span class="token punctuation">[</span>j<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> dp<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token keyword">else</span><span class="token punctuation">:</span>dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">if</span> dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token operator">></span><span class="token builtin">max</span><span class="token punctuation">:</span><span class="token builtin">max</span> <span class="token operator">=</span> dp<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">max</span><span class="token punctuation">)</span></code></pre><h3 id="有效括号组合"><a href="#有效括号组合" class="headerlink" title="有效括号组合"></a>有效括号组合</h3><p>数字 n 代表生成括号的对数，设计一个函数，用于能够生成所有可能的并且<strong>有效的</strong>括号组合。</p><p><strong>固定第n个括号的左括号在最左，则n-1个括号可以在n括号内或外</strong>，通过动态规划可以直到小于n时的任意个括号的有效组合</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> n <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>total <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>total<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">)</span>total<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'()'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span>n<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>s <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>now_list1 <span class="token operator">=</span> total<span class="token punctuation">[</span>j<span class="token punctuation">]</span>        <span class="token comment"># p = j 时的括号组合情况</span>now_list2 <span class="token operator">=</span> total<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token operator">-</span>j<span class="token punctuation">]</span>    <span class="token comment"># q = (i-1) - j 时的括号组合情况,因为已经固定了一个</span><span class="token keyword">for</span> k1 <span class="token keyword">in</span> now_list1<span class="token punctuation">:</span><span class="token keyword">for</span> k2 <span class="token keyword">in</span> now_list2<span class="token punctuation">:</span><span class="token keyword">if</span> k1 <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>k1 <span class="token operator">=</span> <span class="token string">''</span><span class="token keyword">if</span> k2 <span class="token operator">==</span> <span class="token boolean">None</span><span class="token punctuation">:</span>k2 <span class="token operator">=</span> <span class="token string">''</span>en <span class="token operator">=</span> <span class="token string">'('</span> <span class="token operator">+</span> k1 <span class="token operator">+</span> <span class="token string">')'</span> <span class="token operator">+</span> k2s<span class="token punctuation">.</span>append<span class="token punctuation">(</span>en<span class="token punctuation">)</span>total<span class="token punctuation">.</span>append<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token keyword">return</span> total<span class="token punctuation">[</span>n<span class="token punctuation">]</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Dynamic Programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LC Slicing of List</title>
      <link href="/2022/05/31/lc-slicing-of-list/"/>
      <url>/2022/05/31/lc-slicing-of-list/</url>
      
        <content type="html"><![CDATA[<h2 id="Slicing-of-List"><a href="#Slicing-of-List" class="headerlink" title="Slicing of List"></a><strong>Slicing of List</strong></h2><pre class="language-none"><code class="language-none">a &#x3D; [0,1,2,3,4,5,6,7,8,9]# 当索引只有一个数时，表示切取某一个元素。&gt;&gt;&gt;a[0]&gt;&gt;&gt;0&gt;&gt;&gt;a[-4]&gt;&gt;&gt;6# 从左往右&gt;&gt;&gt;a[:]&gt;&gt;&gt; [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]# 从左往右&gt;&gt;&gt;a[::]&gt;&gt;&gt; [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]# 从右往左&gt;&gt;&gt;a[::-1]&gt;&gt;&gt; [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]# step&#x3D;1，从左往右取值，start_index&#x3D;1到end_index&#x3D;6同样表示从左往右取值。&gt;&gt;&gt;a[1:6]&gt;&gt;&gt; [1, 2, 3, 4, 5]&gt;&gt;&gt;a[1:6:-1]&gt;&gt;&gt; []# 输出为空列表，说明没取到数据。# step&#x3D;-1，决定了从右往左取值，而start_index&#x3D;1到end_index&#x3D;6决定了从左往右取值，两者矛盾，所以为空。&gt;&gt;&gt;a[6:2]&gt;&gt;&gt; []# 同样输出为空列表。# step&#x3D;1，决定了从左往右取值，而start_index&#x3D;6到end_index&#x3D;2决定了从右往左取值，两者矛盾，所以为空。&gt;&gt;&gt;a[:6]&gt;&gt;&gt; [0, 1, 2, 3, 4, 5]# step&#x3D;1，表示从左往右取值，而start_index省略时，表示从端点开始，因此这里的端点是“起点”，即从“起点”值0开始一直取到end_index&#x3D;6（该点不包括）。&gt;&gt;&gt;a[:6:-1]&gt;&gt;&gt; [9, 8, 7]# step&#x3D;-1，从右往左取值，而start_index省略时，表示从端点开始，因此这里的端点是“终点”，即从“终点”值9开始一直取到end_index&#x3D;6（该点不包括）。&gt;&gt;&gt;a[6:]&gt;&gt;&gt; [6, 7, 8, 9]# step&#x3D;1，从左往右取值，从start_index&#x3D;6开始，一直取到“终点”值9。&gt;&gt;&gt;a[6::-1]&gt;&gt;&gt; [6, 5, 4, 3, 2, 1, 0]# step&#x3D;-1，从右往左取值，从start_index&#x3D;6开始，一直取到“起点”0。</code></pre>]]></content>
      
      
      <categories>
          
          <category> Leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DNN Gradient Vanishing or Exploding</title>
      <link href="/2022/05/30/dnn-gradient-vanishing-or-exploding/"/>
      <url>/2022/05/30/dnn-gradient-vanishing-or-exploding/</url>
      
        <content type="html"><![CDATA[<h2 id="DNN-Gradient-Vanishing-or-Exploding"><a href="#DNN-Gradient-Vanishing-or-Exploding" class="headerlink" title="DNN Gradient Vanishing or Exploding"></a>DNN Gradient Vanishing or Exploding</h2><p>build 2 hidden layer DNN with an output layer</p><p><img src="/vanish_1.png"></p><p>loss is function of f3</p><p><img src="/vanish_2.png"></p><p>根据上面规律，我们可以把x写成f0，当有n-1层隐层时，fn是输出，如果要求wl也就是第l层的权重，反向传播中涉及的偏导计算为：</p><p><img src="/vanish_3.png"></p><p>前半部分是关于激活函数的导数的累乘，后半部分是关于权重值的累乘。</p><p>激活函数如sigmoid函数，其导数的取值范围是(0, 0.25]，当网络层数很深的时候，多个小于1的数进行累乘，结果是趋向于0的，此时梯度反向传播的时候，根据参数更新公式：$$w <em>{l} &#x3D; w</em>{l}-lr·\frac{\partial loss}{\partial w _{l}} $$ ，偏导部分的取值趋于0，那么该参数得不到更新，这就是梯度消失现象。消失的只是更新参数所需要的梯度，而不一定是参数被更新到0。</p><p>关于权重值的累乘，当我们初始化权值很大的时候，多个大于1的数累乘，结果是+∞，此时就出现了梯度爆炸现象。</p><h2 id="NN-detach"><a href="#NN-detach" class="headerlink" title="NN: detach()"></a><strong>NN: detach()</strong></h2><p>detach()是生成了一个新的variable，detach_()是对本身的更改，</p><p>detach()</p><p>（1）返回一个<strong>新的从当前图中分离</strong>的Variable。</p><p>（2）返回的 Variable 不会梯度更新</p><p>（3）被detach()的相当于**with torch.no_grad()**操作</p><p>（4）返回的Variable和被detach的Variable指向同一个tensor</p><pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">2</span><span class="token operator">>></span><span class="token operator">></span> y<span class="token punctuation">.</span>requires_grad<span class="token boolean">False</span></code></pre><h2 id="NN-detach-1"><a href="#NN-detach-1" class="headerlink" title="NN: detach_()"></a><strong>NN: detach_()</strong></h2><p>Detach_()</p><p>（1）将<strong>一个Variable</strong>从创建它的图中分离，并把它设置成<strong>叶子Variable</strong>。</p><p>（2）将中间节点的grad_fn的值设置为None时，这样中间节点就不会再与前一个节点关联，此时的中间节点就变成了叶子节点。</p><p>（3）将中间节点的requires_grad设置为False，这样对后面节点进行backward()时就不会对中间节点求梯度</p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DNN </tag>
            
            <tag> Gradient </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dropout</title>
      <link href="/2022/05/29/dropout/"/>
      <url>/2022/05/29/dropout/</url>
      
        <content type="html"><![CDATA[<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a><strong>Dropout</strong></h2><p>一般出现在<strong>全连接层</strong>后，防止模型过拟合。</p><p>神经网络的输入单元是否归零服从<strong>伯努利分布</strong>，并以概率p随机地将神经网络的某个单元的输出（对下一层而言是输入）置为0。</p><p>在训练中，每个隐层的神经元先乘以概率P，然后再进行激活。</p><p>在测试中，所有的神经元先进行激活，然后每个隐层神经元的输出乘P。</p><p><img src="/2022/05/29/dropout/dropout.jpg"></p> <pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">NeuralNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>NeuralNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>         self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>         self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token comment"># dropout训练</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        <span class="token keyword">return</span> out    model <span class="token operator">=</span> NeuralNet<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span></code></pre><h3 id="启用Batch-Normalization-和-Dropout"><a href="#启用Batch-Normalization-和-Dropout" class="headerlink" title="启用Batch Normalization 和 Dropout"></a>启用Batch Normalization 和 Dropout</h3><p>需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。</p><h3 id="不启用-Batch-Normalization-和-Dropout"><a href="#不启用-Batch-Normalization-和-Dropout" class="headerlink" title="不启用 Batch Normalization 和 Dropout"></a>不启用 Batch Normalization 和 Dropout</h3><p>如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout, model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。</p><p>训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的的性质。</p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dropout </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN</title>
      <link href="/2022/05/28/cnn/"/>
      <url>/2022/05/28/cnn/</url>
      
        <content type="html"><![CDATA[<h2 id="Convolutional-Layer"><a href="#Convolutional-Layer" class="headerlink" title="Convolutional Layer"></a><strong>Convolutional Layer</strong></h2><p>Make a convolutional module:</p> <pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">torch</span><span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span>                       stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># 前三个参数必填</span>mod <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">2</span> <span class="token punctuation">,</span> <span class="token number">5</span> <span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span></code></pre><p>inputs: 2 channels ——every channel is a feature map, 灰度图片只有一个feature map；彩色图片一般3个feature map（RGB）</p><p>output: 5 activation maps —— 5个激活映射，每个filter都会产生一个，所以有5个filters</p><p>filters are 3x3, padding with one layer of zero to not shrink anything</p><h2 id="Receptive-Field"><a href="#Receptive-Field" class="headerlink" title="Receptive Field"></a><strong>Receptive Field</strong></h2><p>感受野（Receptive Field）的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。再通俗点的解释是，特征图上的一个点对应输入图上的区域，如图。</p><p>![Receptive Field](receptive field.jpg)</p><p>两个3*3卷积层的串联相当于1个5*5的卷积层，3个3*3的卷积层串联相当于1个7*7的卷积层，即<strong>3个3*3卷积层的感受野大小相当于1个7*7的卷积层</strong>。但是3个3*3的卷积层参数量只有<strong>7*7</strong>的一半左右，同时前者可以有<strong>3个</strong>非线性操作，而后者<strong>只有1个</strong>非线性操作，这样使得前者对于特征的学习能力更强。</p><h2 id="Pooling-Layer"><a href="#Pooling-Layer" class="headerlink" title="Pooling Layer"></a><strong>Pooling Layer</strong></h2><p>Make a pooling module</p><p>inputs: activation maps of size n x n</p><p>output: activation maps of size n&#x2F;p x n&#x2F;p</p><p>p: pooling size, in this case p &#x3D;&#x3D; 2</p> <pre class="language-python" data-language="python"><code class="language-python">mod <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>    <span class="token comment"># max pooling</span><span class="token comment"># block 1: 3 x 32 x 32 --> 64 x 16 x 16    </span>self<span class="token punctuation">.</span>conv1a <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>self<span class="token punctuation">.</span>conv1b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>self<span class="token punctuation">.</span>pool1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span></code></pre><h2 id="Unpooling-layer"><a href="#Unpooling-layer" class="headerlink" title="Unpooling layer"></a><strong>Unpooling layer</strong></h2><p>![Bed of Nails](Bed of Nails.jpg)</p><p>![Nearest Neighbor](Nearest Neighbor.jpg)</p><p>![Max Unpooling](Max Unpooling.jpg)</p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> Pooling and Unpooling </tag>
            
            <tag> Receptive Field </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vanilla Neural Networks</title>
      <link href="/2022/05/28/vanilla-neural-networks/"/>
      <url>/2022/05/28/vanilla-neural-networks/</url>
      
        <content type="html"><![CDATA[<h2 id="torch-softmax-x2F-torch-nn-Softmax"><a href="#torch-softmax-x2F-torch-nn-Softmax" class="headerlink" title="torch.softmax() &#x2F; torch.nn.Softmax()"></a><strong>torch.softmax() &#x2F; torch.nn.Softmax()</strong></h2><p><img src="/2022/05/28/vanilla-neural-networks/1_softmax.jpg" alt="softmax"></p><pre class="language-python" data-language="python"><code class="language-python">A <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span> <span class="token punctuation">[</span> <span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">2</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token punctuation">,</span> <span class="token number">2.0</span> <span class="token punctuation">,</span> <span class="token number">1.5</span> <span class="token punctuation">]</span> <span class="token punctuation">,</span>  <span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">2</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token punctuation">,</span> <span class="token number">7.0</span> <span class="token punctuation">,</span> <span class="token number">1.5</span> <span class="token punctuation">]</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>p <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>A <span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>              <span class="token comment"># over rows</span><span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token comment"># tensor([[1.0730e-02, 4.8089e-02, 5.8585e-01, 3.5533e-01], </span><span class="token comment">#         [1.2282e-04, 5.5046e-04, 9.9526e-01, 4.0674e-03]])</span>p <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>A <span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># over columns</span><span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token comment"># tensor([[0.5000, 0.5000, 0.0067, 0.5000],</span><span class="token comment">#         [0.5000, 0.5000, 0.9933, 0.5000]])</span><span class="token comment"># torch.nn.LogSoftmax() is taking logarithms of the result of softmax(xi)</span><span class="token comment"># also we have torch.nn.Softmax()</span></code></pre><p>x is stored in one column, although it is represented in one line when typing it on the console.</p><pre class="language-python" data-language="python"><code class="language-python">x<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">2</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token punctuation">,</span> <span class="token number">2.0</span> <span class="token punctuation">,</span> <span class="token number">1.5</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>p<span class="token operator">=</span>torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear()"></a><strong>nn.Linear()</strong></h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># input of size  5 and output of size 3</span>mod <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>bias <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>         <span class="token comment"># bias = True by default</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">)</span><span class="token comment"># Linear(in_features=5, out_features=3, bias=True)</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">.</span>weight<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># Parameter containing:</span><span class="token comment"># tensor([[-0.0636, 0.1377, -0.1297, 0.4385, 0.1840],</span><span class="token comment"># [-0.4137, 0.2118, 0.2093, -0.0728, -0.2257],</span><span class="token comment"># [ 0.4318, -0.1557, 0.1055, 0.3528, 0.2025]], requires_grad=True)</span><span class="token comment"># torch.Size([3, 5])</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">.</span>bias<span class="token punctuation">)</span><span class="token comment"># Parameter containing:</span><span class="token comment"># tensor([ 0.1466, 0.2684, -0.0493], requires_grad=True)</span> <span class="token comment"># change the weight of mod</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    mod<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>    mod<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>     mod<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">.</span>weight<span class="token punctuation">)</span></code></pre><h2 id="vanilla-nn"><a href="#vanilla-nn" class="headerlink" title="vanilla nn"></a><strong>vanilla nn</strong></h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">two_layer_net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token builtin">super</span><span class="token punctuation">(</span>two_layer_net <span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span> input_size<span class="token punctuation">,</span> hidden_size <span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span> hidden_size<span class="token punctuation">,</span> output_size <span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>          <span class="token comment"># relu activation function</span>x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>p <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span>           <span class="token comment"># can be modified to meet your demands</span><span class="token keyword">return</span> p net <span class="token operator">=</span> two_layer_net<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token comment"># Alternatively, all the parameters of the network can be accessed # by **net.parameters()**.</span>list_of_param <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>list_of_param<span class="token punctuation">)</span><span class="token comment"># [Parameter containing:</span><span class="token comment">#tensor([[10.0000, 20.0000],</span><span class="token comment">#        [ 0.0500, -0.5119],</span><span class="token comment">#        [-0.1930, -0.1993],</span><span class="token comment">#        [-0.0208, -0.0490],</span><span class="token comment">#        [ 0.2011, -0.2519]], requires_grad=True),</span><span class="token comment"># Parameter containing:</span><span class="token comment"># tensor([ 0.1292, -0.3313, -0.3548, -0.5247, 0.1753], requires_grad=True), </span><span class="token comment"># Parameter containing:</span><span class="token comment"># tensor([[0.3178, -0.1838, -0.1930, -0.3816, 0.1850],</span><span class="token comment">#         [ 0.2342, -0.2743, 0.2424, -0.3598, 0.3090],</span><span class="token comment">#         [ 0.0876, -0.3785, 0.2032, -0.2937, 0.0382]], requires_grad=True), </span><span class="token comment">#Parameter containing:</span><span class="token comment"># tensor([ 0.2120, -0.2751, 0.2351], requires_grad=True)]</span></code></pre><h2 id="nn-NLLLoss"><a href="#nn-NLLLoss" class="headerlink" title="nn.NLLLoss()"></a><strong>nn.NLLLoss()</strong></h2><p>负对数似然损失函数(Negtive Log Likehood) </p><pre class="language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>nll <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>target1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>target2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>target3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>n1 <span class="token operator">=</span> nll<span class="token punctuation">(</span>a<span class="token punctuation">,</span>target1<span class="token punctuation">)</span><span class="token comment"># tensor(-1.)</span>n2 <span class="token operator">=</span> nll<span class="token punctuation">(</span>a<span class="token punctuation">,</span>target2<span class="token punctuation">)</span><span class="token comment"># tensor(-2.)</span>n3 <span class="token operator">=</span> nll<span class="token punctuation">(</span>a<span class="token punctuation">,</span>target3<span class="token punctuation">)</span><span class="token comment"># tensor(-3.)</span></code></pre><p>nn.NLLLoss()取出a中对应target位置的值并取负号，比如target[1]&#x3D;&#x3D;0，就取a中index&#x3D;0位置上的值再取负，作为NLLLoss的输出</p><h2 id="nn-CrossEntropyLoss"><a href="#nn-CrossEntropyLoss" class="headerlink" title="nn.CrossEntropyLoss()"></a><strong>nn.CrossEntropyLoss()</strong></h2><p><img src="/2022/05/28/vanilla-neural-networks/2_crossentropyloss.jpg" alt="Cross Entropy Loss"></p><pre class="language-python" data-language="python"><code class="language-python">mycrit<span class="token operator">=</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>labels<span class="token operator">=</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>scores<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span> <span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.7</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.3</span><span class="token punctuation">,</span> <span class="token number">5.0</span><span class="token punctuation">]</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>average_loss <span class="token operator">=</span> mycrit<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>labels<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'loss = '</span><span class="token punctuation">,</span> average_loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token comment"># loss = 0.023508397862315178</span></code></pre><h2 id="CrossEntropyLoss-x3D-Softmax-Log-NLL"><a href="#CrossEntropyLoss-x3D-Softmax-Log-NLL" class="headerlink" title="CrossEntropyLoss &#x3D; Softmax + Log + NLL"></a><strong>CrossEntropyLoss &#x3D; Softmax + Log + NLL</strong></h2><pre class="language-python" data-language="python"><code class="language-python">softmax_func<span class="token operator">=</span>nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>soft_output<span class="token operator">=</span>softmax_func<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>log_output<span class="token operator">=</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>soft_output<span class="token punctuation">)</span>nllloss_func<span class="token operator">=</span>nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>nllloss_output<span class="token operator">=</span>nllloss_func<span class="token punctuation">(</span>log_output<span class="token punctuation">,</span> y_target<span class="token punctuation">)</span></code></pre><h2 id="epoch"><a href="#epoch" class="headerlink" title="epoch"></a><strong>epoch</strong></h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment"># Do 15 passes through the training set</span>shuffled_indices<span class="token operator">=</span>torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">60000</span><span class="token punctuation">)</span><span class="token keyword">for</span> count <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">60000</span><span class="token punctuation">,</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># indices is a tensor, the following is slicing</span>indices<span class="token operator">=</span>shuffled_indices<span class="token punctuation">[</span>count<span class="token punctuation">:</span>count<span class="token operator">+</span>bs<span class="token punctuation">]</span>minibatch_data <span class="token operator">=</span> train_data<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>minibatch_label <span class="token operator">=</span> train_label<span class="token punctuation">[</span>indices<span class="token punctuation">]</span><span class="token comment"># pay attention on how to slice a tensor</span></code></pre><h2 id="epoch-monitoring-loss-time-lr-update"><a href="#epoch-monitoring-loss-time-lr-update" class="headerlink" title="epoch + monitoring loss + time + lr update"></a><strong>epoch + monitoring loss + time + lr update</strong></h2><pre class="language-python" data-language="python"><code class="language-python">start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>lr <span class="token operator">=</span> <span class="token number">0.05</span>   \# initial learning rate<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># learning rate strategy : divide the learning rate by 1.5 every 10 epochs</span><span class="token keyword">if</span> epoch<span class="token operator">%</span><span class="token number">10</span><span class="token operator">==</span><span class="token number">0</span> <span class="token keyword">and</span> epoch<span class="token operator">></span><span class="token number">10</span><span class="token punctuation">:</span> lr <span class="token operator">=</span> lr <span class="token operator">/</span> <span class="token number">1.5</span><span class="token comment"># create a new optimizer at the beginning of each epoch: give the current learning rate.</span>optimizer<span class="token operator">=</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">,</span> lr<span class="token operator">=</span>lr <span class="token punctuation">)</span>running_loss<span class="token operator">=</span><span class="token number">0</span>running_error<span class="token operator">=</span><span class="token number">0</span>num_batches<span class="token operator">=</span><span class="token number">0</span>shuffled_indices<span class="token operator">=</span>torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">60000</span><span class="token punctuation">)</span><span class="token keyword">for</span> count <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">60000</span><span class="token punctuation">,</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># Set the gradients to zeros</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># create a minibatch </span>        indices<span class="token operator">=</span>shuffled_indices<span class="token punctuation">[</span>count<span class="token punctuation">:</span>count<span class="token operator">+</span>bs<span class="token punctuation">]</span>        minibatch_data <span class="token operator">=</span> train_data<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>        minibatch_label<span class="token operator">=</span> train_label<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>        <span class="token comment"># send them to the gpu</span>        device <span class="token operator">=</span> torch<span class="token punctuation">.</span> device<span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>        net <span class="token operator">=</span> net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        minibatch_data<span class="token operator">=</span>minibatch_data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        minibatch_label<span class="token operator">=</span>minibatch_label<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        <span class="token comment"># reshape the minibatch</span>        inputs <span class="token operator">=</span> minibatch_data<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span><span class="token number">784</span><span class="token punctuation">)</span>        <span class="token comment"># tell Pytorch to start tracking all operations that will be done on "inputs"</span>        inputs<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># forward the minibatch through the net </span>        scores<span class="token operator">=</span>net<span class="token punctuation">(</span> inputs <span class="token punctuation">)</span>         <span class="token comment"># Compute the average of the losses of the data points in the minibatch</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span> scores <span class="token punctuation">,</span> minibatch_label<span class="token punctuation">)</span>         <span class="token comment"># backward pass to compute dL/dU, dL/dV and dL/dW </span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># START COMPUTING STATS</span>        <span class="token comment"># add the loss of this batch to the running loss</span>        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># compute the error made on this batch and add it to the running error  </span>        error <span class="token operator">=</span> utils<span class="token punctuation">.</span>get_error<span class="token punctuation">(</span> scores<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">,</span> minibatch_label<span class="token punctuation">)</span>        running_error <span class="token operator">+=</span> error<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        num_batches<span class="token operator">+=</span><span class="token number">1</span>        <span class="token comment"># compute some stats</span>        running_loss <span class="token operator">+=</span> loss<span class="token operator">**</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">**</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        error <span class="token operator">=</span> utils<span class="token punctuation">.</span>get_error<span class="token punctuation">(</span> scores<span class="token operator">**</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">**</span> <span class="token punctuation">,</span> minibatch_label<span class="token punctuation">)</span>        running_error <span class="token operator">+=</span> error<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        num_batches<span class="token operator">+=</span><span class="token number">1</span>    <span class="token comment"># once the epoch is finished we divide the "running quantities"</span>    <span class="token comment"># by the number of batches</span>    total_loss <span class="token operator">=</span> running_loss<span class="token operator">/</span>num_batches    total_error <span class="token operator">=</span> running_error<span class="token operator">/</span>num_batches    elapsed_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> – start    <span class="token comment"># every 10 epoch we display the stats </span>    <span class="token comment"># and compute the error rate on the test set </span>    <span class="token keyword">if</span> epoch <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token punctuation">:</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch='</span><span class="token punctuation">,</span>epoch<span class="token punctuation">,</span> <span class="token string">' time='</span><span class="token punctuation">,</span> elapsed_time<span class="token punctuation">,</span><span class="token string">' loss='</span><span class="token punctuation">,</span> total_loss <span class="token punctuation">,</span> <span class="token string">' error='</span><span class="token punctuation">,</span> total_error<span class="token operator">*</span><span class="token number">100</span> <span class="token punctuation">,</span><span class="token string">'percent lr='</span><span class="token punctuation">,</span> lr<span class="token punctuation">)</span>eval_on_test_set<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="Evaluate-on-test-set"><a href="#Evaluate-on-test-set" class="headerlink" title="Evaluate on test set"></a><strong>Evaluate on test set</strong></h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">eval_on_test_set</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>running_error<span class="token operator">=</span><span class="token number">0</span>num_batches<span class="token operator">=</span><span class="token number">0</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">10000</span><span class="token punctuation">,</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>        minibatch_data <span class="token operator">=</span> test_data<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>bs<span class="token punctuation">]</span>        minibatch_label<span class="token operator">=</span> test_label<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>bs<span class="token punctuation">]</span>        inputs <span class="token operator">=</span> minibatch_data<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span><span class="token number">784</span><span class="token punctuation">)</span>        scores<span class="token operator">=</span>net<span class="token punctuation">(</span> inputs <span class="token punctuation">)</span>         error <span class="token operator">=</span> utils<span class="token punctuation">.</span>get_error<span class="token punctuation">(</span> scores <span class="token punctuation">,</span> minibatch_label<span class="token punctuation">)</span>        running_error <span class="token operator">+=</span> error<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        num_batches<span class="token operator">+=</span><span class="token number">1</span>    total_error <span class="token operator">=</span> running_error<span class="token operator">/</span>num_batches<span class="token keyword">print</span><span class="token punctuation">(</span> <span class="token string">'test error = '</span><span class="token punctuation">,</span> total_error<span class="token operator">*</span><span class="token number">100</span> <span class="token punctuation">,</span><span class="token string">'percent'</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Vanilla NN </tag>
            
            <tag> Softmax </tag>
            
            <tag> Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction of Tensors</title>
      <link href="/2022/05/27/introduction-of-tensors/"/>
      <url>/2022/05/27/introduction-of-tensors/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Reshaping-a-tensor"><a href="#1-Reshaping-a-tensor" class="headerlink" title="1.Reshaping a tensor"></a><strong>1.Reshaping a tensor</strong></h2><pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) </span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>          <span class="token comment"># two rows with five columns</span><span class="token comment"># tensor([[0, 1, 2, 3, 4],</span>          <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>Note that the original x is not modified, but p &#x3D; x.view(2,5) can store the change in variable p.</p><h2 id="2-Entries-of-a-tensor-can-be-scalar"><a href="#2-Entries-of-a-tensor-can-be-scalar" class="headerlink" title="2.Entries of a tensor can be scalar"></a><strong>2.Entries of a tensor can be scalar</strong></h2><p>A matrix is 2-dimensional Tensor</p><p>A row of a matrix is a 1-dimensional Tensor</p><p>An entry of a matrix is a 0-dimensional Tensor</p><p>0-dimensional Tensor are <strong>scalar</strong></p><p>If we want to convert a 0-dimensional Tensor into python number, we need to use <strong>item()</strong></p> <pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token comment"># how to slice a tensor: x[0], x[1:4], etc.</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># tensor(3.)</span><span class="token comment"># &lt;class 'torch.Tensor'></span><span class="token comment"># 3.0</span><span class="token comment"># &lt;class 'float'></span></code></pre><h2 id="3-The-Storage-of-Tensors"><a href="#3-The-Storage-of-Tensors" class="headerlink" title="3.The Storage of Tensors"></a><strong>3.The Storage of Tensors</strong></h2><pre class="language-python" data-language="python"><code class="language-python">A <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span> <span class="token comment"># tensor([0, 1, 2, 3, 4])</span>B <span class="token operator">=</span> A<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>storage<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data_ptr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>B<span class="token punctuation">.</span>storage<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data_ptr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 2076006947200</span><span class="token comment"># 2076006947200</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tensors </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
