<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CNN</title>
      <link href="/2022/05/29/cnn/"/>
      <url>/2022/05/29/cnn/</url>
      
        <content type="html"><![CDATA[<h2 id="Convolutional-Layer"><a href="#Convolutional-Layer" class="headerlink" title="Convolutional Layer"></a><strong>Convolutional Layer</strong></h2><p>Make a convolutional module:</p> <pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">torch</span><span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span>                       stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># 前三个参数必填</span>mod <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">2</span> <span class="token punctuation">,</span> <span class="token number">5</span> <span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span></code></pre><p>inputs: 2 channels ——every channel is a feature map, 灰度图片只有一个feature map；彩色图片一般3个feature map（RGB）</p><p>output: 5 activation maps —— 5个激活映射，每个filter都会产生一个，所以有5个filters</p><p>filters are 3x3, padding with one layer of zero to not shrink anything</p><h2 id="Receptive-Field"><a href="#Receptive-Field" class="headerlink" title="Receptive Field"></a><strong>Receptive Field</strong></h2><p>感受野（Receptive Field）的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。再通俗点的解释是，特征图上的一个点对应输入图上的区域，如图。</p><p><img src="/2022/05/29/cnn/clip_image002.jpg" alt="Receptive Field"></p><p>两个3*3卷积层的串联相当于1个5*5的卷积层，3个3*3的卷积层串联相当于1个7*7的卷积层，即<strong>3个3*3卷积层的感受野大小相当于1个7*7的卷积层</strong>。但是3个3*3的卷积层参数量只有<strong>7*7</strong>的一半左右，同时前者可以有<strong>3个</strong>非线性操作，而后者<strong>只有1个</strong>非线性操作，这样使得前者对于特征的学习能力更强。</p><h2 id="Pooling-Layer"><a href="#Pooling-Layer" class="headerlink" title="Pooling Layer"></a><strong>Pooling Layer</strong></h2><p>Make a pooling module</p><p>inputs: activation maps of size n x n</p><p>output: activation maps of size n&#x2F;p x n&#x2F;p</p><p>p: pooling size, in this case p &#x3D;&#x3D; 2</p> <pre class="language-python" data-language="python"><code class="language-python">mod <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>    <span class="token comment"># max pooling</span><span class="token comment"># block 1: 3 x 32 x 32 --> 64 x 16 x 16    </span>self<span class="token punctuation">.</span>conv1a <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>self<span class="token punctuation">.</span>conv1b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span>self<span class="token punctuation">.</span>pool1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span></code></pre><h2 id="Unpooling-layer"><a href="#Unpooling-layer" class="headerlink" title="Unpooling layer"></a><strong>Unpooling layer</strong></h2><p><img src="/2022/05/29/cnn/clip_image004.jpg" alt="Bed of Nails"></p><p><img src="/2022/05/29/cnn/clip_image006.jpg" alt="Nearest Neighbor"></p><p><img src="/2022/05/29/cnn/clip_image008.jpg" alt="Max Unpooling"></p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> Pooling and Unpooling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vanilla Neural Networks</title>
      <link href="/2022/05/28/vanilla-neural-networks/"/>
      <url>/2022/05/28/vanilla-neural-networks/</url>
      
        <content type="html"><![CDATA[<h2 id="torch-softmax-x2F-torch-nn-Softmax"><a href="#torch-softmax-x2F-torch-nn-Softmax" class="headerlink" title="torch.softmax() &#x2F; torch.nn.Softmax()"></a><strong>torch.softmax() &#x2F; torch.nn.Softmax()</strong></h2><p><img src="/2022/05/28/vanilla-neural-networks/1_softmax.jpg" alt="softmax"></p><pre class="language-python" data-language="python"><code class="language-python">A <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span> <span class="token punctuation">[</span> <span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">2</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token punctuation">,</span> <span class="token number">2.0</span> <span class="token punctuation">,</span> <span class="token number">1.5</span> <span class="token punctuation">]</span> <span class="token punctuation">,</span>  <span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">2</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token punctuation">,</span> <span class="token number">7.0</span> <span class="token punctuation">,</span> <span class="token number">1.5</span> <span class="token punctuation">]</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>p <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>A <span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>              <span class="token comment"># over rows</span><span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token comment"># tensor([[1.0730e-02, 4.8089e-02, 5.8585e-01, 3.5533e-01], </span><span class="token comment">#         [1.2282e-04, 5.5046e-04, 9.9526e-01, 4.0674e-03]])</span>p <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>A <span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment"># over columns</span><span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token comment"># tensor([[0.5000, 0.5000, 0.0067, 0.5000],</span><span class="token comment">#         [0.5000, 0.5000, 0.9933, 0.5000]])</span><span class="token comment"># torch.nn.LogSoftmax() is taking logarithms of the result of softmax(xi)</span><span class="token comment"># also we have torch.nn.Softmax()</span></code></pre><p>x is stored in one column, although it is represented in one line when typing it on the console.</p><pre class="language-python" data-language="python"><code class="language-python">x<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token operator">-</span><span class="token number">2</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span> <span class="token punctuation">,</span> <span class="token number">2.0</span> <span class="token punctuation">,</span> <span class="token number">1.5</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>p<span class="token operator">=</span>torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear()"></a><strong>nn.Linear()</strong></h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token comment"># input of size  5 and output of size 3</span>mod <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>bias <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>         <span class="token comment"># bias = True by default</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">)</span><span class="token comment"># Linear(in_features=5, out_features=3, bias=True)</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">.</span>weight<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># Parameter containing:</span><span class="token comment"># tensor([[-0.0636, 0.1377, -0.1297, 0.4385, 0.1840],</span><span class="token comment"># [-0.4137, 0.2118, 0.2093, -0.0728, -0.2257],</span><span class="token comment"># [ 0.4318, -0.1557, 0.1055, 0.3528, 0.2025]], requires_grad=True)</span><span class="token comment"># torch.Size([3, 5])</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">.</span>bias<span class="token punctuation">)</span><span class="token comment"># Parameter containing:</span><span class="token comment"># tensor([ 0.1466, 0.2684, -0.0493], requires_grad=True)</span> <span class="token comment"># change the weight of mod</span><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    mod<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>    mod<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>     mod<span class="token punctuation">.</span>weight<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span><span class="token keyword">print</span><span class="token punctuation">(</span>mod<span class="token punctuation">.</span>weight<span class="token punctuation">)</span></code></pre><h2 id="vanilla-nn"><a href="#vanilla-nn" class="headerlink" title="vanilla nn"></a><strong>vanilla nn</strong></h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">two_layer_net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token builtin">super</span><span class="token punctuation">(</span>two_layer_net <span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span> input_size<span class="token punctuation">,</span> hidden_size <span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>self<span class="token punctuation">.</span>layer2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span> hidden_size<span class="token punctuation">,</span> output_size <span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>          <span class="token comment"># relu activation function</span>x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>p <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span>           <span class="token comment"># can be modified to meet your demands</span><span class="token keyword">return</span> p net <span class="token operator">=</span> two_layer_net<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token comment"># Alternatively, all the parameters of the network can be accessed # by **net.parameters()**.</span>list_of_param <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>list_of_param<span class="token punctuation">)</span></code></pre><p># [Parameter containing:<br>#tensor([[10.0000, 20.0000],</p><p>​              [ 0.0500, -0.5119],</p><p>​              [-0.1930, -0.1993],</p><p>​              [-0.0208, -0.0490],</p><p>​              [ 0.2011, -0.2519]], requires_grad&#x3D;True),# Parameter containing:</p><p># tensor([ 0.1292, -0.3313, -0.3548, -0.5247, 0.1753], requires_grad&#x3D;True), </p><p># Parameter containing:</p><p># tensor([[ 0.3178, -0.1838, -0.1930, -0.3816, 0.1850],</p><p>​               [ 0.2342, -0.2743, 0.2424, -0.3598, 0.3090],</p><p>​               [ 0.0876, -0.3785, 0.2032, -0.2937, 0.0382]], requires_grad&#x3D;True), #Parameter containing:</p><p># tensor([ 0.2120, -0.2751, 0.2351], requires_grad&#x3D;True)]</p><h2 id="nn-NLLLoss"><a href="#nn-NLLLoss" class="headerlink" title="nn.NLLLoss()"></a><strong>nn.NLLLoss()</strong></h2><p>负对数似然损失函数(Negtive Log Likehood) </p><pre class="language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>nll <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>target1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>target2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>target3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">(</span><span class="token punctuation">)</span>n1 <span class="token operator">=</span> nll<span class="token punctuation">(</span>a<span class="token punctuation">,</span>target1<span class="token punctuation">)</span><span class="token comment"># tensor(-1.)</span>n2 <span class="token operator">=</span> nll<span class="token punctuation">(</span>a<span class="token punctuation">,</span>target2<span class="token punctuation">)</span><span class="token comment"># tensor(-2.)</span>n3 <span class="token operator">=</span> nll<span class="token punctuation">(</span>a<span class="token punctuation">,</span>target3<span class="token punctuation">)</span><span class="token comment"># tensor(-3.)</span></code></pre><p>nn.NLLLoss()取出a中对应target位置的值并取负号，比如target[1]&#x3D;&#x3D;0，就取a中index&#x3D;0位置上的值再取负，作为NLLLoss的输出</p><h2 id="nn-CrossEntropyLoss"><a href="#nn-CrossEntropyLoss" class="headerlink" title="nn.CrossEntropyLoss()"></a><strong>nn.CrossEntropyLoss()</strong></h2><p><img src="/2022/05/28/vanilla-neural-networks/2_crossentropyloss.jpg" alt="Cross Entropy Loss"></p><pre class="language-python" data-language="python"><code class="language-python">mycrit<span class="token operator">=</span>nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>labels<span class="token operator">=</span>torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>scores<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span> <span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.4</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.7</span> <span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.3</span><span class="token punctuation">,</span> <span class="token number">5.0</span><span class="token punctuation">]</span> <span class="token punctuation">]</span><span class="token punctuation">)</span>average_loss <span class="token operator">=</span> mycrit<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>labels<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'loss = '</span><span class="token punctuation">,</span> average_loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token comment"># loss = 0.023508397862315178</span></code></pre><h2 id="CrossEntropyLoss-x3D-Softmax-Log-NLL"><a href="#CrossEntropyLoss-x3D-Softmax-Log-NLL" class="headerlink" title="CrossEntropyLoss &#x3D; Softmax + Log + NLL"></a><strong>CrossEntropyLoss &#x3D; Softmax + Log + NLL</strong></h2><pre class="language-python" data-language="python"><code class="language-python">softmax_func<span class="token operator">=</span>nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>soft_output<span class="token operator">=</span>softmax_func<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>log_output<span class="token operator">=</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>soft_output<span class="token punctuation">)</span>nllloss_func<span class="token operator">=</span>nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>nllloss_output<span class="token operator">=</span>nllloss_func<span class="token punctuation">(</span>log_output<span class="token punctuation">,</span> y_target<span class="token punctuation">)</span></code></pre><h2 id="epoch"><a href="#epoch" class="headerlink" title="epoch"></a><strong>epoch</strong></h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment"># Do 15 passes through the training set</span>shuffled_indices<span class="token operator">=</span>torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">60000</span><span class="token punctuation">)</span><span class="token keyword">for</span> count <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">60000</span><span class="token punctuation">,</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># indices is a tensor, the following is slicing</span>indices<span class="token operator">=</span>shuffled_indices<span class="token punctuation">[</span>count<span class="token punctuation">:</span>count<span class="token operator">+</span>bs<span class="token punctuation">]</span>minibatch_data <span class="token operator">=</span> train_data<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>minibatch_label <span class="token operator">=</span> train_label<span class="token punctuation">[</span>indices<span class="token punctuation">]</span><span class="token comment"># pay attention on how to slice a tensor</span></code></pre><h2 id="epoch-monitoring-loss-time-lr-update"><a href="#epoch-monitoring-loss-time-lr-update" class="headerlink" title="epoch + monitoring loss + time + lr update"></a><strong>epoch + monitoring loss + time + lr update</strong></h2><pre class="language-python" data-language="python"><code class="language-python">start <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>lr <span class="token operator">=</span> <span class="token number">0.05</span>   \# initial learning rate<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># learning rate strategy : divide the learning rate by 1.5 every 10 epochs</span><span class="token keyword">if</span> epoch<span class="token operator">%</span><span class="token number">10</span><span class="token operator">==</span><span class="token number">0</span> <span class="token keyword">and</span> epoch<span class="token operator">></span><span class="token number">10</span><span class="token punctuation">:</span> lr <span class="token operator">=</span> lr <span class="token operator">/</span> <span class="token number">1.5</span><span class="token comment"># create a new optimizer at the beginning of each epoch: give the current learning rate.</span>optimizer<span class="token operator">=</span>torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">,</span> lr<span class="token operator">=</span>lr <span class="token punctuation">)</span>running_loss<span class="token operator">=</span><span class="token number">0</span>running_error<span class="token operator">=</span><span class="token number">0</span>num_batches<span class="token operator">=</span><span class="token number">0</span>shuffled_indices<span class="token operator">=</span>torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">60000</span><span class="token punctuation">)</span><span class="token keyword">for</span> count <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">60000</span><span class="token punctuation">,</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment"># Set the gradients to zeros</span>optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># create a minibatch </span>        indices<span class="token operator">=</span>shuffled_indices<span class="token punctuation">[</span>count<span class="token punctuation">:</span>count<span class="token operator">+</span>bs<span class="token punctuation">]</span>        minibatch_data <span class="token operator">=</span> train_data<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>        minibatch_label<span class="token operator">=</span> train_label<span class="token punctuation">[</span>indices<span class="token punctuation">]</span>        <span class="token comment"># send them to the gpu</span>        device <span class="token operator">=</span> torch<span class="token punctuation">.</span> device<span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>        net <span class="token operator">=</span> net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        minibatch_data<span class="token operator">=</span>minibatch_data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        minibatch_label<span class="token operator">=</span>minibatch_label<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        <span class="token comment"># reshape the minibatch</span>        inputs <span class="token operator">=</span> minibatch_data<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span><span class="token number">784</span><span class="token punctuation">)</span>        <span class="token comment"># tell Pytorch to start tracking all operations that will be done on "inputs"</span>        inputs<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># forward the minibatch through the net </span>        scores<span class="token operator">=</span>net<span class="token punctuation">(</span> inputs <span class="token punctuation">)</span>         <span class="token comment"># Compute the average of the losses of the data points in the minibatch</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span> scores <span class="token punctuation">,</span> minibatch_label<span class="token punctuation">)</span>         <span class="token comment"># backward pass to compute dL/dU, dL/dV and dL/dW </span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># do one step of stochastic gradient descent: U=U-lr(dL/dU), V=V-lr(dL/dU), ...</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># START COMPUTING STATS</span>        <span class="token comment"># add the loss of this batch to the running loss</span>        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment"># compute the error made on this batch and add it to the running error  </span>        error <span class="token operator">=</span> utils<span class="token punctuation">.</span>get_error<span class="token punctuation">(</span> scores<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">,</span> minibatch_label<span class="token punctuation">)</span>        running_error <span class="token operator">+=</span> error<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        num_batches<span class="token operator">+=</span><span class="token number">1</span>        <span class="token comment"># compute some stats</span>        running_loss <span class="token operator">+=</span> loss<span class="token operator">**</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">**</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        error <span class="token operator">=</span> utils<span class="token punctuation">.</span>get_error<span class="token punctuation">(</span> scores<span class="token operator">**</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">**</span> <span class="token punctuation">,</span> minibatch_label<span class="token punctuation">)</span>        running_error <span class="token operator">+=</span> error<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        num_batches<span class="token operator">+=</span><span class="token number">1</span>    <span class="token comment"># once the epoch is finished we divide the "running quantities"</span>    <span class="token comment"># by the number of batches</span>    total_loss <span class="token operator">=</span> running_loss<span class="token operator">/</span>num_batches    total_error <span class="token operator">=</span> running_error<span class="token operator">/</span>num_batches    elapsed_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> – start    <span class="token comment"># every 10 epoch we display the stats </span>    <span class="token comment"># and compute the error rate on the test set </span>    <span class="token keyword">if</span> epoch <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token punctuation">:</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch='</span><span class="token punctuation">,</span>epoch<span class="token punctuation">,</span> <span class="token string">' time='</span><span class="token punctuation">,</span> elapsed_time<span class="token punctuation">,</span><span class="token string">' loss='</span><span class="token punctuation">,</span> total_loss <span class="token punctuation">,</span> <span class="token string">' error='</span><span class="token punctuation">,</span> total_error<span class="token operator">*</span><span class="token number">100</span> <span class="token punctuation">,</span><span class="token string">'percent lr='</span><span class="token punctuation">,</span> lr<span class="token punctuation">)</span>eval_on_test_set<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h2 id="Evaluate-on-test-set"><a href="#Evaluate-on-test-set" class="headerlink" title="Evaluate on test set"></a><strong>Evaluate on test set</strong></h2><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">eval_on_test_set</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>running_error<span class="token operator">=</span><span class="token number">0</span>num_batches<span class="token operator">=</span><span class="token number">0</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">10000</span><span class="token punctuation">,</span>bs<span class="token punctuation">)</span><span class="token punctuation">:</span>        minibatch_data <span class="token operator">=</span> test_data<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>bs<span class="token punctuation">]</span>        minibatch_label<span class="token operator">=</span> test_label<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>bs<span class="token punctuation">]</span>        inputs <span class="token operator">=</span> minibatch_data<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span><span class="token number">784</span><span class="token punctuation">)</span>        scores<span class="token operator">=</span>net<span class="token punctuation">(</span> inputs <span class="token punctuation">)</span>         error <span class="token operator">=</span> utils<span class="token punctuation">.</span>get_error<span class="token punctuation">(</span> scores <span class="token punctuation">,</span> minibatch_label<span class="token punctuation">)</span>        running_error <span class="token operator">+=</span> error<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>        num_batches<span class="token operator">+=</span><span class="token number">1</span>    total_error <span class="token operator">=</span> running_error<span class="token operator">/</span>num_batches<span class="token keyword">print</span><span class="token punctuation">(</span> <span class="token string">'test error = '</span><span class="token punctuation">,</span> total_error<span class="token operator">*</span><span class="token number">100</span> <span class="token punctuation">,</span><span class="token string">'percent'</span><span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Vanilla NN </tag>
            
            <tag> Softmax </tag>
            
            <tag> Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN</title>
      <link href="/2022/05/28/dropout/"/>
      <url>/2022/05/28/dropout/</url>
      
        <content type="html"><![CDATA[<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a><strong>Dropout</strong></h2><p>一般出现在<strong>全连接层</strong>后，防止模型过拟合。</p><p>神经网络的输入单元是否归零服从<strong>伯努利分布</strong>，并以概率p随机地将神经网络的某个单元的输出（对下一层而言是输入）置为0。</p><p>在训练中，每个隐层的神经元先乘以概率P，然后再进行激活。</p><p>在测试中，所有的神经元先进行激活，然后每个隐层神经元的输出乘P。</p><p><img src="/clip_image002.jpg" alt="img"></p> <pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">NeuralNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>NeuralNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>         self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>         self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token comment"># dropout训练</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        <span class="token keyword">return</span> out    model <span class="token operator">=</span> NeuralNet<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span></code></pre><h3 id="启用BN层-Batch-Normalization-和-Dropout"><a href="#启用BN层-Batch-Normalization-和-Dropout" class="headerlink" title="启用BN层(Batch Normalization) 和 Dropout"></a>启用BN层(Batch Normalization) 和 Dropout</h3><p>需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。</p><h3 id="不启用-Batch-Normalization-和-Dropout"><a href="#不启用-Batch-Normalization-和-Dropout" class="headerlink" title="不启用 Batch Normalization 和 Dropout"></a>不启用 Batch Normalization 和 Dropout</h3><p>如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout, model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。</p><p>训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的的性质。</p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Dropout </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction of Tensors</title>
      <link href="/2022/05/27/introduction-of-tensors/"/>
      <url>/2022/05/27/introduction-of-tensors/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Reshaping-a-tensor"><a href="#1-Reshaping-a-tensor" class="headerlink" title="1.Reshaping a tensor"></a><strong>1.Reshaping a tensor</strong></h2><pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) </span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>          <span class="token comment"># two rows with five columns</span><span class="token comment"># tensor([[0, 1, 2, 3, 4],</span>          <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre><p>Note that the original x is not modified, but p &#x3D; x.view(2,5) can store the change in variable p.</p><h2 id="2-Entries-of-a-tensor-can-be-scalar"><a href="#2-Entries-of-a-tensor-can-be-scalar" class="headerlink" title="2.Entries of a tensor can be scalar"></a><strong>2.Entries of a tensor can be scalar</strong></h2><p>A matrix is 2-dimensional Tensor</p><p>A row of a matrix is a 1-dimensional Tensor</p><p>An entry of a matrix is a 0-dimensional Tensor</p><p>0-dimensional Tensor are <strong>scalar</strong></p><p>If we want to convert a 0-dimensional Tensor into python number, we need to use <strong>item()</strong></p> <pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token comment"># how to slice a tensor: x[0], x[1:4], etc.</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># tensor(3.)</span><span class="token comment"># &lt;class 'torch.Tensor'></span><span class="token comment"># 3.0</span><span class="token comment"># &lt;class 'float'></span></code></pre><h2 id="3-The-Storage-of-Tensors"><a href="#3-The-Storage-of-Tensors" class="headerlink" title="3.The Storage of Tensors"></a><strong>3.The Storage of Tensors</strong></h2><pre class="language-python" data-language="python"><code class="language-python">A <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span> <span class="token comment"># tensor([0, 1, 2, 3, 4])</span>B <span class="token operator">=</span> A<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>storage<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data_ptr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>B<span class="token punctuation">.</span>storage<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data_ptr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 2076006947200</span><span class="token comment"># 2076006947200</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tensors </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
