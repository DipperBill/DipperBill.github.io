<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Vanilla Neural Networks</title>
      <link href="/2022/05/28/vanilla-neural-networks/vanilla-neural-networks/"/>
      <url>/2022/05/28/vanilla-neural-networks/vanilla-neural-networks/</url>
      
        <content type="html"><![CDATA[<h2 id="torch-softmax-x2F-torch-nn-Softmax"><a href="#torch-softmax-x2F-torch-nn-Softmax" class="headerlink" title="torch.softmax() &#x2F; torch.nn.Softmax()"></a><strong>torch.softmax() &#x2F; torch.nn.Softmax()</strong></h2><p><img src="/1_3_softmax.jpg" alt="img"></p><p>A &#x3D; torch.Tensor( [ [ -2 , -0.5 , 2.0 , 1.5 ] ,<br>                                [ -2 , -0.5 , 7.0 , 1.5 ] ])</p><p>p &#x3D; torch.softmax(A , <strong>dim&#x3D;1</strong>)              # over rows</p><p>print(p)</p><p># tensor([[1.0730e-02, 4.8089e-02, 5.8585e-01, 3.5533e-01],<br>                [1.2282e-04, 5.5046e-04, 9.9526e-01, 4.0674e-03]])</p><p>p &#x3D; torch.softmax(A , <strong>dim&#x3D;0</strong>) # over columns</p><p>print(p)</p><p># tensor([[0.5000, 0.5000, 0.0067, 0.5000],</p><p>​[0.5000, 0.5000, 0.9933, 0.5000]])</p><p># <strong>torch.nn.LogSoftmax()</strong> is taking logarithms of the result of softmax(xi)</p><p># also we have torch.nn.Softmax()</p><p>x&#x3D;torch.Tensor([ -2 , -0.5 , 2.0 , 1.5 ])</p><p>p&#x3D;torch.softmax(x, <strong>dim &#x3D;</strong> <strong>0</strong>)</p><p>print(p)</p><p>print(p.size())</p><p># prove that x is stored in one column, although it is represented in one line when typing it on the console.</p><h2 id="nn-Linear"><a href="#nn-Linear" class="headerlink" title="nn.Linear()"></a><strong>nn.Linear()</strong></h2><p><strong>mod &#x3D; nn.Linear(5,3,bias &#x3D; True)</strong> # bias &#x3D; True by default</p><p>print(mod)</p><p># Linear(in_features&#x3D;5, out_features&#x3D;3, bias&#x3D;True)</p><p># input of size  5 and output of size 3</p><p>print(mod.weight)</p><p>print(mod.weight.size())</p><p># Parameter containing:</p><p># tensor([[-0.0636, 0.1377, -0.1297, 0.4385, 0.1840],</p><p>   [-0.4137, 0.2118, 0.2093, -0.0728, -0.2257],</p><p>   [ 0.4318, -0.1557, 0.1055, 0.3528, 0.2025]], requires_grad&#x3D;True)</p><p># <strong>torch.Size([3, 5])</strong></p><p>print(mod.bias)</p><p># Parameter containing:</p><p># tensor([ 0.1466, 0.2684, -0.0493], requires_grad&#x3D;True)</p><p># change the weight of mod</p><p>with torch.no_grad():</p><p>​mod.weight[0,0] &#x3D; 0</p><p>​mod.weight[0,1] &#x3D; 1 </p><p>​mod.weight[0,2] &#x3D; 2</p><p>print(mod.weight)</p><h2 id="vanilla-nn"><a href="#vanilla-nn" class="headerlink" title="vanilla nn"></a><strong>vanilla nn</strong></h2><p>class two_layer_net(nn.Module):</p><p>​    def <strong>init</strong>(self, input_size, hidden_size, output_size):</p><p>​super(two_layer_net , self).<strong>init</strong>()</p><p>​<strong>self</strong>.layer1 &#x3D; nn.Linear( input_size, hidden_size , bias&#x3D;True)</p><p>​<strong>self</strong>.layer2 &#x3D; nn.Linear( hidden_size, output_size , bias&#x3D;True)  </p><p>​def forward(self, x):</p><p>​x &#x3D; <strong>self</strong>.layer1(x)</p><p>​x &#x3D; torch.relu(x) # relu activation function</p><p>​x &#x3D; <strong>self</strong>.layer2(x)</p><p>​p &#x3D; torch.softmax(x, <strong>dim&#x3D; 0</strong>)           # can be modified to meet your demands</p><p>​return p</p><p> net &#x3D; two_layer_net(2,5,3)</p><p># Alternatively, all the parameters of the network can be accessed # by <strong>net.parameters()</strong>.</p><p>list_of_param &#x3D; list( net.parameters() )</p><p>print(list_of_param)</p><p># [Parameter containing:</p><p># tensor([[10.0000, 20.0000],</p><p>  [ 0.0500, -0.5119],</p><p>  [-0.1930, -0.1993],</p><p>  [-0.0208, -0.0490],</p><p>  [ 0.2011, -0.2519]], requires_grad&#x3D;True),# Parameter containing:</p><p># tensor([ 0.1292, -0.3313, -0.3548, -0.5247, 0.1753], requires_grad&#x3D;True), </p><p># Parameter containing:</p><p># tensor([[ 0.3178, -0.1838, -0.1930, -0.3816, 0.1850],</p><p>  [ 0.2342, -0.2743, 0.2424, -0.3598, 0.3090],</p><p>  [ 0.0876, -0.3785, 0.2032, -0.2937, 0.0382]], requires_grad&#x3D;True), #Parameter containing:</p><p>#tensor([ 0.2120, -0.2751, 0.2351], requires_grad&#x3D;True)]</p><h2 id="train-vanilla-nn-with-mini-batch"><a href="#train-vanilla-nn-with-mini-batch" class="headerlink" title="train vanilla nn with mini batch"></a><strong>train vanilla nn with mini batch</strong></h2><p>class one_layer_net(nn.Module):</p><p>​def <strong>init</strong>(self, input_size, output_size):</p><p>​super(one_layer_net , self).<strong>init</strong>()</p><p>​self.linear_layer &#x3D; nn.Linear( input_size, output_size , bias&#x3D;False)</p><p>​def forward(self, x):</p><p>​y &#x3D; self.linear_layer(x)</p><p>​prob &#x3D; torch.softmax(y, <strong>dim &#x3D;</strong> <strong>1</strong>)</p><p>​return prob</p><p>net&#x3D;one_layer_net(784,10)</p><p>criterion &#x3D; nn.NLLLoss()</p><p>optimizer&#x3D;torch.optim.SGD(net.parameters() , lr&#x3D;0.01 )</p><p>bs &#x3D; 200</p><p>for iter in range(1,5000): # only 5,000 iterations</p><p>​# Set dL&#x2F;dU, dL&#x2F;dV, dL&#x2F;dW to be filled with zeros</p><p>​optimizer.zero_grad()</p><p>​# create a minibatch</p><p>​<strong># pay special attention on the way of using [indices]</strong></p><p>​<strong># indices:longtensor, so it can be used as train_data[indices]</strong></p><p>​<strong># if indices is a tensor, it cannot be used as train_data[indices]</strong></p><p>​indices&#x3D;torch.LongTensor(bs).random_(0,60000)</p><p>​minibatch_data &#x3D; train_data[indices]</p><p>​minibatch_label&#x3D; train_label[indices]</p><p>​# reshape the minibatch</p><p>​inputs &#x3D; minibatch_data.view(bs,784)</p><p>​</p><p>​# tell Pytorch to start tracking all operations that will be done on “inputs”</p><p>​inputs.requires_grad_()</p><p>​# forward the minibatch through the net </p><p>​scores&#x3D;net( inputs )</p><p>​# Compute the average of the losses of the data points in the minibatch</p><p>​loss &#x3D; criterion( scores , minibatch_label) </p><p>​# backward pass to compute dL&#x2F;dU, dL&#x2F;dV and dL&#x2F;dW </p><p>​loss.backward()</p><p>​# do one step of stochastic gradient descent: U&#x3D;U-lr(dL&#x2F;dU), V&#x3D;V-lr(dL&#x2F;dU), …</p><p>​optimizer.step()</p><h2 id="nn-NLLLoss"><a href="#nn-NLLLoss" class="headerlink" title="nn.NLLLoss()"></a><strong>nn.NLLLoss()</strong></h2><p># 负对数似然损失函数(Negtive Log Likehood) </p><p>a &#x3D; torch.Tensor([[1,2,3]])</p><p>nll &#x3D; nn.NLLLoss()</p><p>target1 &#x3D; torch.Tensor([0]).long()</p><p>target2 &#x3D; torch.Tensor([1]).long()</p><p>target3 &#x3D; torch.Tensor([2]).long()</p><p>n1 &#x3D; nll(a,target1)</p><p># tensor(-1.)</p><p>n2 &#x3D; nll(a,target2)</p><p># tensor(-2.)</p><p>n3 &#x3D; nll(a,target3)</p><p># tensor(-3.)</p><p># nn.NLLLoss()取出a中对应target位置的值并取负号，比如target[1]&#x3D;&#x3D;0，就取a中index&#x3D;0位置上的值再取负，作为NLLLoss的输出</p><h2 id="nn-CrossEntropyLoss"><a href="#nn-CrossEntropyLoss" class="headerlink" title="nn.CrossEntropyLoss()"></a><strong>nn.CrossEntropyLoss()</strong></h2><p><img src="/1_8_crossentropyloss.jpg" alt="img"></p><p>mycrit&#x3D;nn.CrossEntropyLoss()</p><p>labels&#x3D;torch.LongTensor([2,3])</p><p>scores&#x3D;torch.Tensor([ [-1.2, 0.5 , 5, -0.5], </p><p>​[1.4, -1.7 , -1.3, 5.0] ])</p><p>average_loss &#x3D; mycrit(scores,labels)</p><p>print(‘loss &#x3D; ‘, average_loss.item() )</p><p># loss &#x3D; 0.023508397862315178</p><h2 id="CrossEntropyLoss-x3D-Softmax-Log-NLL"><a href="#CrossEntropyLoss-x3D-Softmax-Log-NLL" class="headerlink" title="CrossEntropyLoss &#x3D; Softmax + Log + NLL"></a><strong>CrossEntropyLoss &#x3D; Softmax + Log + NLL</strong></h2><p>softmax_func&#x3D;nn.Softmax(dim&#x3D;1)</p><p>soft_output&#x3D;softmax_func(input)</p><p>log_output&#x3D;torch.log(soft_output)</p><p>nllloss_func&#x3D;nn.NLLLoss(reduction&#x3D;’none’)</p><p>nllloss_output&#x3D;nllloss_func(log_output, y_target)</p><h2 id="epoch"><a href="#epoch" class="headerlink" title="epoch"></a><strong>epoch</strong></h2><p><strong>for</strong> <strong>epoch</strong> <strong>in</strong> range(15): # Do 15 passes through the training set</p><p>​shuffled_indices&#x3D;torch.randperm(60000)</p><p>​for count in range(0,60000,bs):</p><p>​optimizer.zero_grad()</p><p>​# indices is a tensor, the following is slicing</p><p>​<strong>indices&#x3D;shuffled_indices[count:count+bs]</strong></p><p>​minibatch_data &#x3D; train_data[indices]</p><p>​minibatch_label &#x3D; train_label[indices]</p><p>​# pay attention on how to slice a tensor</p><h2 id="epoch-monitoring-loss-time-lr-update"><a href="#epoch-monitoring-loss-time-lr-update" class="headerlink" title="epoch + monitoring loss + time + lr update"></a><strong>epoch + monitoring loss + time + lr update</strong></h2><p>start &#x3D; time.time()</p><p>lr &#x3D; 0.05   # initial learning rate</p><p>for epoch in range(200):</p><p>​# learning rate strategy : divide the learning rate by 1.5 every 10 epochs</p><p>​if epoch%10&#x3D;&#x3D;0 and epoch&gt;10: </p><p>​lr &#x3D; lr &#x2F; 1.5</p><p>​# create a new optimizer at the beginning of each epoch: give the current learning rate.</p><p>​optimizer&#x3D;torch.optim.SGD( net.parameters() , lr&#x3D;lr )</p><p>​running_loss&#x3D;0</p><p>​running_error&#x3D;0</p><p>​num_batches&#x3D;0</p><p>​shuffled_indices&#x3D;torch.randperm(60000)</p><p>​for count in range(0,60000,bs):</p><p>​# Set the gradients to zeros</p><p>​optimizer.zero_grad()</p><p>​# create a minibatch </p><p>​indices&#x3D;shuffled_indices[count:count+bs]</p><p>​minibatch_data &#x3D; train_data[indices]</p><p>​minibatch_label&#x3D; train_label[indices]</p><p>​# send them to the gpu</p><p>​# device &#x3D; torch. device&#x3D; torch.device(“cuda”)</p><p>​# net &#x3D; net.to(device)</p><pre><code>     \# minibatch_data=minibatch_data.to(device)      \# minibatch_label=minibatch_label.to(device)     </code></pre><p>​# reshape the minibatch</p><p>​inputs &#x3D; minibatch_data.view(bs,784)</p><p>​# tell Pytorch to start tracking all operations that will be done on “inputs”</p><p>​inputs.requires_grad_()</p><p>​# forward the minibatch through the net </p><p>​scores&#x3D;net( inputs ) </p><p>​# Compute the average of the losses of the data points in the minibatch</p><p>​loss &#x3D; criterion( scores , minibatch_label) </p><p>​# backward pass to compute dL&#x2F;dU, dL&#x2F;dV and dL&#x2F;dW </p><p>​loss.backward()</p><p>​# do one step of stochastic gradient descent: U&#x3D;U-lr(dL&#x2F;dU), V&#x3D;V-lr(dL&#x2F;dU), …</p><p>​optimizer.step()</p><p>​# START COMPUTING STATS</p><p>​# add the loss of this batch to the running loss</p><p>​running_loss +&#x3D; loss.detach().item()</p><p>​# compute the error made on this batch and add it to the running error  </p><p>​error &#x3D; utils.get_error( scores.detach() , minibatch_label)</p><p>​running_error +&#x3D; error.item()</p><p>​num_batches+&#x3D;1</p><p>​# compute some stats</p><p>​running_loss +&#x3D; loss**.detach()**.item()</p><p>​error &#x3D; utils.get_error( scores**.detach()** , minibatch_label)</p><p>​running_error +&#x3D; error.item()</p><p>​num_batches+&#x3D;1</p><p>​</p><pre><code> \# once the epoch is finished we divide the &quot;running quantities&quot;</code></pre><p>​# by the number of batches</p><p>​total_loss &#x3D; running_loss&#x2F;num_batches</p><p>​total_error &#x3D; running_error&#x2F;num_batches</p><p>​elapsed_time &#x3D; time.time() – start</p><p>​# every 10 epoch we display the stats </p><p>​# and compute the error rate on the test set </p><p>​if epoch % 10 &#x3D;&#x3D; 0 : </p><p>​print(‘epoch&#x3D;’,epoch, ‘ time&#x3D;’, elapsed_time,</p><p>​           ‘ loss&#x3D;’, total_loss , ‘ error&#x3D;’, total_error*100 ,’percent lr&#x3D;’, lr)</p><p>​eval_on_test_set()</p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Vanilla NN </tag>
            
            <tag> Softmax </tag>
            
            <tag> Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction of Tensors</title>
      <link href="/2022/05/27/introduction-of-tensors/"/>
      <url>/2022/05/27/introduction-of-tensors/</url>
      
        <content type="html"><![CDATA[<h2 id="Reshaping-a-tensor"><a href="#Reshaping-a-tensor" class="headerlink" title="Reshaping a tensor"></a><strong>Reshaping a tensor</strong></h2><pre class="language-python" data-language="python"><code class="language-python">x<span class="token operator">=</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></code></pre><p>Note that the original x is not modified, but p &#x3D; x.view(2,5) can store the change in variable p.</p> <pre class="language-pytho" data-language="pytho"><code class="language-pytho">print(x.view(2,5))          # two rows with five columns# tensor([[0, 1, 2, 3, 4],          [5, 6, 7, 8, 9]])</code></pre><h2 id="Entries-of-a-tensor-can-be-scalar"><a href="#Entries-of-a-tensor-can-be-scalar" class="headerlink" title="Entries of a tensor can be scalar"></a><strong>Entries of a tensor can be scalar</strong></h2><p>A matrix is 2-dimensional Tensor</p><p>A row of a matrix is a 1-dimensional Tensor</p><p>An entry of a matrix is a 0-dimensional Tensor</p><p>0-dimensional Tensor are <strong>scalar</strong></p><p>If we want to convert a 0-dimensional Tensor into python number, we need to use <strong>item()</strong></p> <pre class="language-python" data-language="python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>a <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token comment"># how to slice a tensor: x[0], x[1:4], etc.</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># tensor(3.)</span><span class="token comment"># &lt;class 'torch.Tensor'></span><span class="token comment"># 3.0</span><span class="token comment"># &lt;class 'float'></span></code></pre><h2 id="The-Storage-of-Tensors"><a href="#The-Storage-of-Tensors" class="headerlink" title="The Storage of Tensors"></a><strong>The Storage of Tensors</strong></h2><pre class="language-python" data-language="python"><code class="language-python">A <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span> <span class="token comment"># tensor([0, 1, 2, 3, 4])</span>B <span class="token operator">=</span> A<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>storage<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data_ptr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>B<span class="token punctuation">.</span>storage<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data_ptr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 2076006947200</span><span class="token comment"># 2076006947200</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tensors </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
